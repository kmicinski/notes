tifact

Matthew Might

st

eu

OPL *
* P se * Consi

at e d

Michael D. Adams

* Easy to
ed
R
nt

Steven Lyde

alu

Thomas Gilray

e

Pushdown Control-Flow Analysis for Free

* Complete

*

t
en

A EC * E
* Well Docum v

Ar

David Van Horn

arXiv:1507.03137v2 [cs.PL] 21 Mar 2016

University of Utah, USA
University of Maryland, USA
{tgilray,lyde,adamsmd,might}@cs.utah.edu, dvanhorn@cs.umd.edu

Abstract

To avoid this imprecision, Vardoulakis and Shivers [16] introduces a context-free approach (as in context-free languages, not
context sensitivity) to program analysis with CFA2. This technique provides a computable, although exponential-time, method
for obtaining perfect stack precision for monovariant analyses of
continuation-passing-style programs. Two other approaches, PDCFA and AAC, build on this work by enabling polyvariant (e.g.,
context sensitive) analysis of direct-style programs and do so at
only a polynomial-factor increase to the run-time complexity of the
underlying analysis.
Earl et al. [4] presents a pushdown control-flow analysis (PDCFA), which improves on traditional control-flow analysis by annotating edges in the state graph with stack actions (i.e., push and
pop) that implicitly represent precise call stacks. But, this method
obtains its precision at a substantial increase in worst-case complexity. For example, a monovariant PDCFA is in O(n6 ) where its
finite-state equivalent is in O(n3 ). Unfortunately, PDCFA also requires significant machinery and presents challenges to engineers
responsible for constructing and maintaining such analyses.
Johnson and Van Horn [8] presents abstracting abstract control
(AAC), a refinement of store-allocated continuations with the established finite-state method of merging stack frames into the store,
and defines an allocator that is precise enough to avoid all spurious
merging. The key advantage of this method is that it is trivial to
implement in existing analysis frameworks that use store-allocated
continuations and comes at the cost of changing roughly one line of
code. Unfortunately, AAC is more computationally complex than
PDCFA as even in the monovariant case it is in O(n8 ).
We draw on the lessons learned from all three approaches and
present a technique for obtaining perfect call-stack precision at only
a constant-factor increase to run-time complexity over traditional
finite-state analysis (i.e., for free in terms of complexity) and requiring no refactoring of analyses already using store-allocated continuations (i.e., for free in terms of labor).

Traditional control-flow analysis (CFA) for higher-order languages
introduces spurious connections between callers and callees, and
different invocations of a function may pollute each otherâ€™s return flows. Recently, three distinct approaches have been published
that provide perfect call-stack precision in a computable manner:
CFA2, PDCFA, and AAC. Unfortunately, implementing CFA2 and
PDCFA requires significant engineering effort. Furthermore, all
three are computationally expensive. For a monovariant analysis,
CFA2 is in O(2n ), PDCFA is in O(n6 ), and AAC is in O(n8 ).
In this paper, we describe a new technique that builds on these
but is both straightforward to implement and computationally inexpensive. The crucial insight is an unusual state-dependent allocation strategy for the addresses of continuations. Our technique
imposes only a constant-factor overhead on the underlying analysis and costs only O(n3 ) in the monovariant case. We present the
intuitions behind this development, benchmarks demonstrating its
efficacy, and a proof of the precision of this analysis.
Categories and Subject Descriptors D.3.4 [Programming Languages]: Processors and Optimization
Keywords Static analysis; Control-flow analysis; Abstract interpretation; Pushdown analysis; Store-allocated continuations

1.

Introduction

Recent developments in the static analysis of higher-order languages make it possible to obtain perfect precision in modeling the
call stack. This allows calls and returns to be matched up precisely
and avoids spurious return flows. Consider the following Racket
code, which binds an identity function and applies it on two distinct values:
(let* ([id (lambda (x) x)]
[y (id #t)]
[z (id #f)])
...)

1.1

Contributions

We contribute an efficient method for obtaining a perfectly precise
modeling of the call stack in static analyses. Specifically:

Without a precise modeling of the call stack, the value #f can
spuriously flow to the variable y, even when a technique like call
sensitivity initially keeps them separate.

â€¢ We present a novel technique for obtaining perfect call-stack

precision at no asymptotic cost to run-time complexity and
requiring only a trivial change to analyses already using storeallocated continuations. In the monovariant case, our analysis
is in O(n3 ), the same complexity class as a traditional 0-CFA.
â€¢ We illustrate the intuition behind our approach and explain why

previous PTIME methods (PDCFA and AAC) fail to exploit it.

Copyright Â© ACM, 2016. This is the authorâ€™s version of the work. It is posted here
by permission of ACM for your personal use. Not for redistribution. The definitive version was published in POPL â€™16: Proceedings of the 43rd annual ACM
SIGPLAN Symposium on Principles of Programming Languages, January 2016,
http://dx.doi.org/10.1145/2837614.2837631.
PoPL â€™16, January 20â€“22, 2016, St. Petersburg, FL, USA.
Copyright Â© 2016 ACM . . . $15.00.
http://dx.doi.org/10.1145/2837614.2837631

â€¢ We describe our implementation and provide benchmarks that

demonstrate its efficacy.
â€¢ We define a relationship between our technique and a static

analysis that uses unbounded stacks and use it to prove the
precision of our method.

1

1.2

Our concrete interpreter operates over machine states Ï‚.

Outline

Section 2 defines a simple direct-style language and its operational
semantics. It presents the relevant background on abstract interpretation using abstract machines, soundness, store widening, and
concepts necessary to understanding our technique. We close this
section by giving a walkthrough of the above example, illustrating
precisely how values become merged in a traditional analysis.
In section 3, we formalize an incomputable static analysis that
defines what is meant by perfect stack precision. This analysis
loses no precision in its modeling of the call stack but requires
an infinite number of unbounded stacks to be explored. We then
review PDCFA and AAC, the existing polynomial-time approaches
to obtaining an equivalent stack precision.
In section 4, we formalize our technique, give the intuitions
that led us to it, and explain how it relates to each of the analyses
described in section 3. We describe our implementation and present
both monovariant and call-sensitive allocation benchmark results
that compare the complexity and precision of our technique to that
of ACC.
Section 5 provides a formal relationship between the unboundedstack machine of section 3 and our improved finite-state analysis. We use this relationship to prove the perfect precision of our
method.

2.

Ï‚ âˆˆ Î£ , Exp Ã— Env Ã— Store Ã— Kont
Ï âˆˆ Env , Var * Addr

Static analysis by abstract interpretation proves properties of a program by running its code through an interpreter powered by an
abstract semantics that approximates the behavior of a concrete
semantics. This process is a general method for analyzing programs and serves applications such as program verification, malware/vulnerability detection, and compiler optimization, among
others [1â€“3, 10] The abstracting abstract machines (AAM) approach uses abstract interpretation of abstract machines for controlflow analysis (CFA) of functional (higher-order) programming languages [9, 12, 15]. The AAM methodology allows a high degree
of control over how program states are represented and is easy to
instrument.
In this section, we review operational semantics and abstract
interpretation using AAM along with other concepts we will require as we progress. We present a concrete interpretation of a
simple direct-style language, a traditional finite-state abstraction,
and a store-widened polynomial-time analysis. We then explore the
return-flow merging problem in greater detail.

x, y âˆˆ Var is a set of identifiers

[stores]

clo âˆˆ Clo , Lam Ã— Env

[closures]

Îº âˆˆ Kont , Frame âˆ—

[stacks]

Ï† âˆˆ Frame , Var Ã— Exp Ã— Env

[stack frames]

a âˆˆ Addr is an infinite set

[addresses]

A(x, Ï, Ïƒ) , Ïƒ(Ï(x))

[variable lookup]

A(lam, Ï, Ïƒ) , (lam, Ï)

[closure creation]

((let ([y (f Ã¦)]) e), Ï, Ïƒ, Îº)

Î£

(e0 , Ï0 , Ïƒ 0 , Ï† : Îº), where

Ï† = (y, e, Ï)
0

((Î» (x) e ), ÏÎ» ) = A(f, Ï, Ïƒ)
Ï0 = ÏÎ» [x 7â†’ a]
Ïƒ 0 = Ïƒ[a 7â†’ A(Ã¦, Ï, Ïƒ)]
a is a fresh address
A new frame Ï† is pushed onto the stack for eventually returning
to the body of this let-form. The atomic expression f is either a
lambda-form or a variable-reference and is evaluated to a closure
by our helper A. In our notation, ticks are used to uniquely name
identifiers that may be different. These do not have any bearing
on the variableâ€™s domain, but where possible will hint at usage
(e.g., a single tick for a successorâ€™s components). A subscript may
be more significant, but we will be careful to point it out. This is
not the case for ÏÎ» , which is used to name whatever environment
was drawn from the closure for f . This is simply an environment
distinct from Ï and Ï0 . We generate a fresh address a (any address
such that a âˆˆ
/ dom(Ïƒ)) and update ÏÎ» with a mapping x 7â†’ a to
produce the successor environment Ï0 . Likewise, the prior store Ïƒ
is extended at this address with the value for Ã¦ to produce Ïƒ 0 .
Return points transition according to a second rule:

Concrete Semantics

f, Ã¦ âˆˆ AExp ::= x | lam
lam âˆˆ Lam ::= (Î» (x) e)

Ïƒ âˆˆ Store , Addr * Clo

A concrete transition relation ( Î£ ) : Î£ * Î£ defines the
operation of this machine by determining at most one successor
for any given predecessor state. The machine stops when the end
of a programâ€™s execution is reached or when given an invalid state.
Call sites transition according to the following transition rule:

We will be using the direct-style (call-by-value, untyped) Î»calculus in administrative-normal-form (ANF) [6].
e âˆˆ Exp ::= (let ([x (f Ã¦)]) e)
| Ã¦

[environments]

Binding environments (Ï) map variables in scope to a representative address (a). Value stores (Ïƒ) map these addresses to a program
value. (For pure Î»-calculus, all values are closures.) Both are partial functions that are incrementally extended with new points. A
closure (clo) pairs a syntactic lambda with an environment over
which it is closed. Continuations (Îº) are unbounded sequences of
stack frames. Each stack frame (Ï†) contains a variable to bind, an
expression control returns to, and an environment to reinstate. Addresses (a) may be drawn from any set which permits us to generate
an arbitrary number of fresh values (e.g., N).
We define a helper A : AExpÃ—Env Ã—Store * Clo for atomicexpression evaluation:

Background

2.1

[states]

[call]
[return]
[atomic expressions]
[lambda abstractions]
[variables]

(Ã¦, Ï, Ïƒ, Ï† : Îº)

All intermediate expressions are administratively let-bound, and
the order of operations is made explicit as a stack of such lets. This
not only simplifies our semantics, but is convenient for analysis as
every intermediate expression can naturally be given a unique identifier. Additional core forms permitting mutation, recursive binding,
conditional branching, tail calls, and primitive operations add complexity, but do not complicate the technique we aim to discuss and
so are left out.

Î£

(e, Ï0 , Ïƒ 0 , Îº), where

Ï† = (x, e, ÏÎº )
Ï0 = ÏÎº [x 7â†’ a]
Ïƒ 0 = Ïƒ[a 7â†’ A(Ã¦, Ï, Ïƒ)]
a is a fresh address
The top stack frame Ï† is decomposed and its environment ÏÎº
extended with a fresh address a to produce Ï0 . Likewise, the store

2

is extended at this address with the value for Ã¦ to produce Ïƒ 0 . The
expression e in the top stack frame is reinstated at Ï0 , and Ïƒ 0 is put
atop the predecessorâ€™s stack tail Îº.
To fully evaluate a program e0 using these transition rules, we
inject it into our state-space using a helper I : Exp â†’ Î£:

threading it through the store as a linked list. A continuation is thus
represented by an address. This address points to a set of topmost
frames, each paired with the address of its continuation in turn (i.e.,
that stackâ€™s tail). We separate the continuation store (ÏƒÌƒÎº ) from the
value store (ÏƒÌƒ) to maintain simplicity as we progress.
Abstract environments (ÏÌƒ) change only because our address set
f are approximate only by virtue
is now finite. Abstract closures (clo)
of their environments using these abstract addresses. For each such
Ëœ of closures. At
aÌƒ, the finite value store (ÏƒÌƒ) denotes a flow set (d)
each point, a continuation store (ÏƒÌƒÎº ) has a set of continuations (kÌƒ).
Like closures, each abstract frame (Ï†Ìƒ) is approximate only by virtue
of its abstracted environment. An abstract continuation (ÎºÌƒ) pairs a
frame with an address (aÌƒÎº ) for the stack underneath.
As before, we define a helper for abstract atomic evaluation, AÌƒ:

I(e) , (e, âˆ…, âˆ…, )
We perform the standard lifting of (
semantics defined over sets of states:

Î£

) to obtain a collecting

s âˆˆ S , P(Î£)
Our collecting relation ( S ) is a monotonic, total function that
gives a set including the trivially reachable state I(e0 ) plus the set
of all states immediately succeeding those in its input.
s

S

s0 , s0 = {Ï‚ 0 | Ï‚ âˆˆ s âˆ§ Ï‚

Î£

Ï‚ 0 } âˆª {I(e0 )}
g Ã— Store
^ * DÌƒ
AÌƒ : AExp Ã— Env

If the program e0 terminates, iteration of ( S ) from âŠ¥ (i.e., the
empty set âˆ…) does as well. That is, ( S )n (âŠ¥) is a fixed point containing e0 â€™s full program trace for some n âˆˆ N whenever e0 is a
terminating program. No such n is guaranteed to exist in the general case (when e0 is a non-terminating program) as our language
(the untyped Î»-calculus) is Turing-complete, our semantics is fully
precise, and the state-space we defined is infinite.
2.2

Abstract Semantics

[environments]
[stores]
[flow-sets]

f âˆˆ Clo
g , Lam Ã— Env
g
clo

[closures]

] â†’ KÌƒ
^ , Addr
ÏƒÌƒÎº âˆˆ KStore
]
kÌƒ âˆˆ KÌƒ , P(Kont)

One such behavior is to simply return the variable itself (as a 0-CFA
would):
] 0 (x, Ï‚Ëœ) , x
alloc
] 0 would tune our finite-state semantics to the monovariUsing alloc
ant analysis style (also called zeroth-order CFA), a form of contextinsensitive analysis. In a monovariant analysis, every closure that is
bound to a variable x at any point during a concrete execution ends
up being represented in a single flow set when the analysis is complete.
Because we are also now store-allocating continuations and distinguishing a top-level continuation store, we likewise distinguish
an abstract allocator specifically for addresses in this store:

A standard choice is to allocate based on the target expression:
] Îº 0 ((e, ÏÌƒ, ÏƒÌƒ, ÏƒÌƒÎº , aÌƒÎº ), e0 , ÏÌƒ0 , ÏƒÌƒ 0 ) , e0
alloc

[kont-sets]
[continuations]

g
^ , Var Ã— Exp Ã— Env
Ï†Ìƒ âˆˆ Frame

[stack frame]

] is a finite set
aÌƒ, aÌƒÎº âˆˆ Addr

g Ã— Store
] Îº : Î£Ìƒ Ã— Exp Ã— Env
]
^ â†’ Addr
alloc

[continuation stores]

]
] , Frame
^ Ã— Addr
ÎºÌƒ âˆˆ Kont

[closure creation]

] : Var Ã— Î£Ìƒ â†’ Addr
]
alloc

]
^ Ã— Addr
Ã— KStore

] â†’ DÌƒ
^ , Addr
ÏƒÌƒ âˆˆ Store
g
dËœ âˆˆ DÌƒ , P(Clo)

[variable lookup]

Note that atomic evaluation of a lambda expression new yields a
set containing a single element for the closure of that lambda.
Because our address domain is now finite, multiple concrete allocations need to be represented by a single abstract address. There
are a variety of sound strategies for doing this. Each strategy corresponds to a distinct style of analysis and is amenable to easy im] helper to encapsulate
plementation by defining an auxiliary alloc
these differences in behavior. Given the variable for which to allocate and the finite state performing the allocation, the abstract
allocator returns an address:

We are now ready to design a computable approximation of the exact program trace using an abstract semantics. Previous work has
explored a wide variety of approaches to systematically abstracting
a semantics like these [9, 12, 15]. Broadly construed, the nature of
these changes is to simultaneously finitize the domains of our machine while introducing non-determinism both into the transition
relation (multiple successor states may immediately follow a predecessor state) and the store (multiple values may be indicated by a
single address). We use a finite state space to ensure computability.
However, to justify that a semantics defined over this finite machine
is soundly approximating our concrete semantics (for a defined notion of abstraction), we must also modify our finite states so that
a potentially infinite number of concrete states may abstract to a
single finite state. We will use this term finite state to differentiate from other kinds of machine states. Components unique to this
finite-state machine wear tildes:
g Ã— Store
^ [states]
Ï‚Ëœ âˆˆ Î£Ìƒ , Exp Ã— Env

g , Var * Addr
]
ÏÌƒ âˆˆ Env

AÌƒ(x, ÏÌƒ, ÏƒÌƒ) , ÏƒÌƒ(ÏÌƒ(x))
AÌƒ(lam, ÏÌƒ, ÏƒÌƒ) , {(lam, ÏÌƒ)}

We provide to this function all the information known about the
transition being made. The value-store allocator is invoked before
a successor ÏÌƒ0 or ÏƒÌƒ 0 is constructed. However, when calling the
continuation-store allocator, we provide information about the target state being transitioned to. The choice of e0 for allocating a
continuation address makes sense considering the entry point of a
function should know where it is returning. In fact, when performing an analysis of a continuation-passing-style (CPS) language, e0
also would naturally be the choice inherited from a monovariant
value-store allocator (assuming an alpha-renaming such that every
x is unique to a single binding point).

[addresses]

There were two fundamental sources of unboundedness in the concrete machine: the value store (with an infinite domain of addresses), and the current continuation (modeled as an unbounded
list of stack frames). We bound the value store (ÏƒÌƒ) by restricting
its domain to a finite set of addresses (aÌƒ), but we permit a set of
f at each. We finitize the stack similarly by
abstract closures (clo)
3

Our collecting relation ( ~S ) is a monotonic, total function that
gives a set including the trivially reachable finite-state IÌƒ(e0 ) plus
the set of all states immediately succeeding those in its input.
Because Î£Ìƒ is now finite, we know the approximate evaluation of
even a non-terminating e0 will terminate. That is, for some n âˆˆ N,
the value ( S~ )n (âŠ¥) is guaranteed to be a fixed point containing an
approximation of e0 â€™s full program trace [14].

We may now define a non-deterministic finite-state transition
relation ( Î£~ ) âŠ† Î£Ìƒ Ã— Î£Ìƒ. Call sites transition as follows.
Ï‚Ìƒ

z
}|
{
((let ([y (f Ã¦)]) e), ÏÌƒ, ÏƒÌƒ, ÏƒÌƒÎº , aÌƒÎº )

~
Î£

(e0 , ÏÌƒ0 , ÏƒÌƒ 0 , ÏƒÌƒÎº0 , aÌƒ0Îº ), where

((Î» (x) e0 ), ÏÌƒÎ» ) âˆˆ AÌƒ(f, ÏÌƒ, ÏƒÌƒ)
ÏÌƒ0 = ÏÌƒÎ» [x 7â†’ aÌƒ]

2.3

ÏƒÌƒ 0 = ÏƒÌƒ t [aÌƒ 7â†’ AÌƒ(Ã¦, ÏÌƒ, ÏƒÌƒ)]
]
aÌƒ = alloc(x,
Ï‚Ëœ)
ÏƒÌƒÎº0 = ÏƒÌƒÎº t [aÌƒ0Îº 7â†’ {((y, e, ÏÌƒ), aÌƒÎºÌ‚ )}]
] Îº (Ëœ
aÌƒ0Îº = alloc
Ï‚ , e0 , ÏÌƒ0 , ÏƒÌƒ 0 )
As AÌƒ yields a set of abstract closures for f , a successor state is
produced for each. Likewise, so each point in the store accumulates
all closures bound at that abstract address aÌƒ and so we faithfully
over-approximate all the addresses a that aÌƒ simulates, we use a
join operation when extending the store. The join of two stores
distributes point-wise as follows.
0

Soundness

An analysis is sound if the information it provides about a program represents an accurate bound on the behavior of all possible concrete executions. The kind of control-flow information the
finite-state analysis in section 2.2 obtains is a conservative overapproximation of program behavior. It places an upper bound on
the propagation of closures though a program.
To establish such a relationship between a concrete and abstract
semantics, we use Galois connections. A Galois connection is a
pair of functions for abstraction and concretization such that the
following holds.
Î± : S â†’ SÌƒ

Î³ : SÌƒ â†’ S

0

ÏƒÌƒ t ÏƒÌƒ , Î»aÌƒ. ÏƒÌƒ(aÌƒ) âˆª ÏƒÌƒ (aÌƒ)
ÏƒÌƒÎº t ÏƒÌƒÎº0 , Î»aÌƒÎº . ÏƒÌƒÎº (aÌƒÎº ) âˆª ÏƒÌƒÎº0 (aÌƒÎº )

Î±(s) âŠ† sÌƒ â‡â‡’ s âŠ† Î³(sÌƒ)

Instead of generating a fresh address for aÌƒ, we use our abstract allocation policy to select one. To instantiate a monovariant analysis
like 0-CFA, this address is simply the syntactic variable x. Likewise, we generate an address for our continuation (a new stack
frame atop the current continuation) and extend the continuation
store.
The return transition is modified in the same way:

Using this defined notion of simulation, we may show that our
abstract semantics approximates the concrete semantics by proving
that simulation is preserved across transition:
Î±(s) âŠ† sÌƒ âˆ§ s

~
Î£

(e, ÏÌƒ0 , ÏƒÌƒ 0 , ÏƒÌƒÎº , aÌƒ0Îº ), where

]
aÌƒ = alloc(x,
Ï‚Ëœ)
Where multiple topmost stack frames are pointed to by aÌƒÎº , this
transition yields multiple successors. An updated environment and
store are produced as before, but the continuation store remains as
it was. The current continuation aÌƒ0Îº reinstated in each successor is
the address associated with each topmost stack frame.
To approximately evaluate a program according to these abstract
semantics, we first define an abstract injection function, IÌƒ, where
the stores begin as functions, âŠ¥, that map every abstract address to
the empty set.

2.4

IÌƒ(e) , (e, âˆ…, âŠ¥, âŠ¥, aÌƒhalt )

^
|Store|

The number of syntactic points in an input program is in O(n). In
the monovariant case, environments map variables to themselves
and are isomorphic to the sets of free variables that may be determined for each syntactic point. The number of addresses produced
by our monovariant allocators is in O(n) as these are either syntactic variables or expressions. The number of value stores may
be visualized as a table of possible mappings from every address
to every abstract closureâ€”each may be included in a given store

sÌƒ âˆˆ SÌƒ , P(Î£Ìƒ)
Î£

^
|KStore|

g|
^|
|Exp|
|Env
|Addr
z}|{
z}|{
z}|{ z}|{
z}|{
n2
n2
O( n Ã— n Ã— 2
Ã— 2
Ã— n )

The address aÌƒhalt can be any otherwise unused address that is never
returned by the allocation function. Our machine will eventually be
unable to transition into this continuation and will then produce no
successors, which simulates the behavior of our concrete machine
upon reaching an empty stack ().
We again lift ( Î£~ ) to obtain a collecting semantics ( ~S ) defined over sets of states:
~

Store Widening

Various forms of widening and further approximations may be
layered on top of this naÃ¯ve analysis. One such approximation is
store widening, which is necessary for our analysis to be tractable
(i.e., polynomial time). To see why store widening is necessary,
let us consider the complexity of an analysis using ( ~S ). The
height of the power-set lattice (SÌƒ, âˆª, âˆ©) is the number of elements
in Î£Ìƒ which is the product of expressions, environments, stores,
and addresses. A standard worklist algorithm at most does work
proportional to the number of states it can discover [13]. For the
imprecise allocators we have defined, analysis run-time is thus in:

IÌƒ : Exp â†’ Î£Ìƒ

sÌƒ0 , sÌƒ0 = {Ëœ
Ï‚ 0 | Ï‚Ëœ âˆˆ sÌƒ âˆ§ Ï‚Ëœ

sÌƒ0 âˆ§ Î±(s0 ) âŠ† sÌƒ0

~

ÏƒÌƒ 0 = ÏƒÌƒ t [aÌƒ 7â†’ AÌƒ(Ã¦, ÏÌƒ, ÏƒÌƒ)]

~

S

âˆ’âˆ’âˆ’Sâˆ’
â†’ sÌƒ0
sÌƒ âˆ’
Both constructing analyses using Galois connections and proving
them sound using Galois connections has been extensively explored in the literature [12, 15]. The analysis style we constructed
in section 2.2 has been previously proven sound using the above
method [11].

ÏÌƒ0 = ÏÌƒÎº [x 7â†’ aÌƒ]

S

~

s âˆ’
âˆ’âˆ’âˆ’Sâˆ’
â†’ s0
ï£¦
ï£¦
ï£¦
ï£¦
âŠ†yÎ±
âŠ†yÎ±

((x, e, ÏÌƒÎº ), aÌƒ0Îº ) âˆˆ ÏƒÌƒÎº (aÌƒÎº )

sÌƒ

s0 =â‡’ sÌƒ

Diagrammatically this is:

Ï‚Ìƒ

z
}|
{
(Ã¦, ÏÌƒ, ÏƒÌƒ, ÏƒÌƒÎº , aÌƒÎº )

S

Ï‚Ëœ0 } âˆª {IÌƒ(e0 )}

4

In this definition, an underscore (wildcard) matches anything. The
height of the RÌƒ lattice is linear (as environments are monovariant)
and the height of the store lattices are quadratic (as each global
store is strictly extended). Each extension of the store may require
O(n) transitions because at any given store, we must transition
every configuration to be sure to obtain any changes to the store
or otherwise reach a fixed point. A traditional worklist algorithm
for computing a fixed point is thus cubic:

O(n)

}|

z
f0
clo
ï£±
ï£®
ï£´
aÌƒ0
0
ï£´
ï£´
ï£´
ï£´
aÌƒ1 ï£¯
1
ï£´
ï£¯
ï£´
ï£² . ï£¯ .
.. ï£¯ ..
O(n)
ï£¯
ï£´
ï£´
aÌƒ
0
ï£´ j ï£¯
ï£´
ï£°
ï£´
ï£´
..
..
ï£´
ï£³ .
.

{

f1
clo

Â·Â·Â·

fi
clo

Â·Â·Â·

0
1
..
.
0
..
.

Â·Â·Â·
Â·Â·Â·
..
.
Â·Â·Â·
..
.

1
0
..
.
1
..
.

Â·Â·Â·
Â·Â·Â·
..
.
Â·Â·Â·
..
.

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

^
|Store|

Figure 1. The value space of stores.
2.5

[state-spaces]

rÌƒ âˆˆ RÌƒ , P(CÌƒ)

[reachable configurations]

g Ã— Addr
]
cÌƒ âˆˆ CÌƒ , Exp Ã— Env

] 1 , consider the following snippet of code where
Now, using alloc
the variable id is already bound to (Î» (x) 0 x):
... 1 (let ([y (id #t)])
2
(let ([z (id #f)])
3
...))
We number these expressions for ease of reference. For example, e2
refers to the let-form that binds z, and e0 to the return point of id.
We assume the starting configuration for this example is (e1 , ÏÌƒ, aÌƒÎº )
where ÏÌƒ and aÌƒÎº are the binding environment and continuation address at the start of this code. We likewise let ÏÌƒÎ» be the environment
of idâ€™s closure.
The first call to id transitions to evaluate e0 with the continuation address e0 . This transition reaches the configuration
(e0 , ÏÌƒÎ» [x 7â†’ (x, e1 )], e0 ) and binds (x, e1 ) to #t and the continuation address e0 to the continuation ((y, e2 , ÏÌƒ), aÌƒÎº ), which gives us
the following stores:
ÏƒÌƒ = {(x, e1 ) 7â†’ {#t}}
ÏƒÌƒÎº = {e0
7â†’ {((y, e2 , ÏÌƒ), aÌƒÎº )}}
Next, id returns and transitions from e0 to e2 , extending the continuationâ€™s environment to ÏÌƒ[y 7â†’ (y, e0 )] and reinstating the
continuation address aÌƒÎº . This yields a configuration (e2 , ÏÌƒ[y 7â†’
(y, e0 )], aÌƒÎº ). This transition binds (y, e0 ) to #t, giving us the following stores:

[configurations]

ÏƒÌƒ = {(x, e1 ) 7â†’ {#t},
(y, e0 ) 7â†’ {#t}}

~

A widened transfer function ( Î ) may then be defined that, like
( S~ ), is a monotonic, total function we may iterate to a fixed point.
(

~
Î

ÏƒÌƒÎº = {e0

) : ÎÌƒ â†’ ÎÌƒ

~
Î

(rÌƒ0 , ÏƒÌƒ 0 , ÏƒÌƒÎº0 ), where

sÌƒ = {Ëœ
Ï‚ | (e, ÏÌƒ, aÌƒÎº ) âˆˆ rÌƒ âˆ§ (e, ÏÌƒ, ÏƒÌƒ, ÏƒÌƒÎº , aÌƒÎº )

ÏƒÌƒ = {(x, e1 ) 7â†’ {#t},

~

Ëœ} âˆª {IÌƒ(e0 )}
Î£ Ï‚

(y, e0 ) 7â†’ {#t},
(x, e2 ) 7â†’ {#f}}

rÌƒ0 = {(e, ÏÌƒ, aÌƒÎº ) | (e, ÏÌƒ, ÏƒÌƒ, ÏƒÌƒÎº , aÌƒÎº ) âˆˆ sÌƒ}
G
G 00
ÏƒÌƒ 0 =
ÏƒÌƒ 00
ÏƒÌƒÎº0 =
ÏƒÌƒÎº
(_,_,ÏƒÌƒ 00 ,_,_)âˆˆsÌƒ

7â†’ {((y, e2 , ÏÌƒ), aÌƒÎº )}}

Then the second call to id transitions to evaluate e0 with the con] Îº 0 ).
tinuation address e0 once again (recall the definition of alloc
This transition reaches the configuration (e0 , ÏÌƒÎ» [x 7â†’ (x, e2 )], e0 ),
binding (x, e2 ) to #f and the continuation address e0 to the continuation ((z, e3 , ÏÌƒ[y 7â†’ (y, e0 )]), aÌƒÎº ), giving us the following stores:

This may be defined in terms of ( ~Î£ ), as was ( S~ ), by transitioning each reachable configuration using the global store to yield a
new set of reachable configurations and a set of stores whose least
upper bound is the new global store:
(rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº )

Stack Imprecision

To illustrate the effect of an imprecise stack on data-flow and
control-flow precision, we first define a more precise 1-callsensitive (first-order, 1-CFA) allocator. A k-call-sensitive analysis
style differentiates bindings to a variable so they are unique to a
history of the last k call sites reached before the binding. A history of length k = 1 then allocates an address unique to the call
site immediately preceeding the binding by using the following
allocator.
] 1 (x, (e, ÏÌƒ, ÏƒÌƒ, ÏƒÌƒÎº , ÎºÌƒ)) , (x, e)
alloc

or not as seen in figure 1. The number of abstract closures is in
O(n) because lambdas uniquely determine a monovariant environment. This times the number of addresses gives O(n2 ) possible additions to the value store. The number of continuations is likewise
in O(n) because let-forms uniquely determine their binding variable, body, and monovariant environment. This times the number
of possible addresses gives O(n2 ) possible additions to the continuation store.
The crux of the issue is that, in exploring a naÃ¯ve state-space
(where each state is specific to a whole store), we may explore both
sides of every diamond in the store lattices. All combinations of
possible bindings in a store may need to be explored, including
every alternate path up the store lattice. For example, along one
f 1 before
explored path we might extend an address aÌƒ1 with clo
f 2 , and along another path we might add these
extending it with clo
f 2 before clo
f 1 ). We might also
closures in the reverse order (i.e., clo
f 1 either before or after either
extend another address aÌƒ2 with clo
of these cases, and so forth. This potential for exponential blowup is unavoidable without further widening or coarser structural
abstraction.
Global-store widening is an essential technique for combating
exponential blow up. This lifts the store alongside a set of reachable
states instead of nesting them inside states in Î£Ìƒ. To formalize this,
we define new widened state spaces that pair a set of reachable
configurations (states sans stores) with a global value store and
global continuation store. Instead of accumulating whole stores,
and thereby all possible sequences of additions within such stores,
the analysis strictly accumulates new values in the store in the same
way ( S~ ) accumulates reachable states in an sÌƒ:
^ Ã— KStore
^
Î¾Ëœ âˆˆ ÎÌƒ , RÌƒ Ã— Store

^
|KStore|

|CÌƒ|
z}|{
z}|{
z}|{
O( n Ã— ( n2 + n2 ))

ÏƒÌƒÎº = {e0

00 ,_)âˆˆsÌƒ
(_,_,_,ÏƒÌƒÎº

5

7â†’ {((y, e2 , ÏÌƒ), aÌƒÎº ) ,
((z, e3 , ÏÌƒ [y 7â†’ (y, e0 )]), aÌƒÎº )}}

Next, id returns and transitions from e0 to e3 , reinstating the continuation address aÌƒÎº and extending the continuationâ€™s environment
to ÏÌƒ[y 7â†’ (y, e0 )][z 7â†’ (z, e0 )]. Because e0 is bound to two continuations, this transition binds (z, e0 ) to #f while another spuriously
binds (y, e0 ) to #f, causing return-flow imprecision in the following stores:

have seen so far, we call this an unbounded-stack machine. Components unique to this machine wear hats:
d Ã— Store
\ Ã— Kont
[
Ï‚Ë† âˆˆ Î£Ì‚ , Exp Ã— Env

ÏƒÌƒ = {(x, e1 ) 7â†’ {#t},
(x, e2 ) 7â†’ {#f},
(y, e0 ) 7â†’ {#t, #f},

ÏƒÌƒÎº = {e0

d , Var * Addr
[
ÏÌ‚ âˆˆ Env

[environments]

[ â†’ DÌ‚
\ , Addr
ÏƒÌ‚ âˆˆ Store
Ë†
d
d âˆˆ DÌ‚ , P(Clo)

[stores]
[flow-sets]

c âˆˆ Clo
d , Lam Ã— Env
d
clo

[closures]

[ , Frame
\
ÎºÌ‚ âˆˆ Kont

(z, e0 ) 7â†’ {#f}}
7â†’ {((y, e2 , ÏÌƒ), aÌƒÎº ) ,
((z, e3 , ÏÌƒ [y 7â†’ (y, e0 )]), aÌƒÎº )}}

[states]

âˆ—

[whole stacks]

d
\ , Var Ã— Exp Ã— Env
Ï†Ì‚ âˆˆ Frame

[stack frames]

[ is a finite set
aÌ‚ âˆˆ Addr

[addresses]

Our atomic-expression evaluator works just as before:
d Ã— Store
\ * DÌ‚
AÌ‚ : AExp Ã— Env

The address (y, e0 ), representing y within e3 , maps to both #t and
#f, even though no concrete execution binds y to #f. A similar
pair of transitions from (e0 , ÏÌƒÎ» [x 7â†’ (x, e1 )], e0 ) (the second of
which is prompted by a change in the global continuation store at
the address e0 ) cause the same conflation for z.
Clearly, one solution is to increase the context sensitivity of our
]Îº1
continuation allocator. Consider a continuation allocator alloc
] 1 uses a single call site of context and allocates
that like alloc
a continuation address (e0 , e) formed from both the expression
being transitioned to, e0 , and the expresson being transitioned from,
e. This results in no spurious merging at return points because
continuations are kept as distinct as the 1-call-sensitive value-store
addresses we allocate.
It seems reasonable from here to suspect that perfect stack
precision could always be obtained through a sufficiently precise
strategy for polyvariant continuation allocation. The difficulty is
in knowing how to obtain this in the general case given an arbitrary value-store allocation strategy. Given that CFA2 and PDCFA
promise a fixed method for implementing perfect stack precision,
albeit at significant engineering and run-time costs, can perfect
stack precision be implemented as a fixed, adaptive continuation
allocator? In this paper, we both answer this question in the affirmative and show that this leads us not only to a trivial implementation
but to only a constant-factor increase in run-time complexity.

3.

[variable lookup]

AÌ‚(lam, ÏÌ‚, ÏƒÌ‚) , {(lam, ÏÌ‚)}

[closure creation]

As does a monovariant allocator:
[ : Var Ã— Î£Ì‚ â†’ Addr
[
alloc
[ 0 (x, Ï‚Ë†) , x
alloc
This may be tuned to any other allocation strategy as easily as
before.
We now define a non-deterministic unbounded-stack-machine
transition relation ( Î£ ) âŠ† Î£Ì‚Ã— Î£Ì‚ and a rule for call-site transitions:
âˆ§

Ï‚Ì‚

z
}|
{
((let ([y (f Ã¦)]) e), ÏÌ‚, ÏƒÌ‚, ÎºÌ‚)

âˆ§

Î£

(e0 , ÏÌ‚0 , ÏƒÌ‚ 0 , Ï†Ì‚ : ÎºÌ‚), where

Ï†Ì‚ = (y, e, ÏÌ‚)
0

((Î» (x) e ), ÏÌ‚Î» ) âˆˆ AÌ‚(f, ÏÌ‚, ÏƒÌ‚)
ÏÌ‚0 = ÏÌ‚Î» [x 7â†’ aÌ‚]
ÏƒÌ‚ 0 = ÏƒÌ‚ t [aÌ‚ 7â†’ AÌ‚(Ã¦, ÏÌ‚, ÏƒÌ‚)]
[
aÌ‚ = alloc(x,
Ï‚Ë†)
This is slightly simplified from its analogue in ( Î£~ ). The definitions of e0 , ÏÌ‚0 , and ÏƒÌ‚ 0 are effectively identical, but the continuation store and continuation address have been replaced with an unbounded stack Ï†Ì‚ : ÎºÌ‚.
Likewise, the return transition also changes to the following.

Perfect Stack Precision

Ï‚Ì‚

We next formalize what is meant by a static analysis with perfect
stack precision by using an abstract abstract machine (AAM) [15]
with unbounded stacks within each machine configuration. We then
review the existing polynomial-time methods for computing an
analysis with equivalent precision to this machine: PDCFA and
AAC.
3.1

AÌ‚(x, ÏÌ‚, ÏƒÌ‚) , ÏƒÌ‚(ÏÌ‚(x))

z
}|
{
(Ã¦, ÏÌ‚, ÏƒÌ‚, Ï†Ì‚ : ÎºÌ‚)

âˆ§

Î£

(e, ÏÌ‚0 , ÏƒÌ‚ 0 , ÎºÌ‚), where

Ï†Ì‚ = (x, e, ÏÌ‚Îº )
ÏÌ‚0 = ÏÌ‚Îº [x 7â†’ aÌ‚]
ÏƒÌ‚ 0 = ÏƒÌ‚ t [aÌ‚ 7â†’ AÌ‚(Ã¦, ÏÌ‚, ÏƒÌ‚)]

Unbounded-Stack Analysis

[
aÌ‚ = alloc(x,
Ï‚Ë†)

In the same manner as previous work on this topic, we formalize
perfect stack precision using a static analysis that leaves the structure of stacks fully unabstracted. Each frame of this unbounded
stack is itself abstract because its environment is abstract and references the abstracted value store. States and configurations, however, directly contain lists of such frames that are unbounded in
length. Environments, closures, stack frames, flow sets, and value
stores are otherwise abstracted in the same manner as the finite machine of section 2.2. To differentiate this from the machines we

To follow a return transition, the stack must contain at least one
frame. Then the appropriate e is reinstated with the environment ÏÌ‚
extended with an address for x. The store is extended and whatever
stack tail existed after Ï†Ì‚ is the successorâ€™s continuation ÎºÌ‚.
Unbounded-state injection is defined as we would expect:
IÌ‚ : Exp â†’ Î£Ì‚
IÌ‚(e) , (e, âˆ…, âŠ¥, )

6

As before, we lift ( Î£ ) to obtain a monotonic naÃ¯ve collecting relation ( S ) for a program e0 that is defined over sets of unboundedstates:

these machines are closely related:

âˆ§

âˆ§

sÌ‚ âˆˆ SÌ‚ , P(Î£Ì‚)
sÌ‚

âˆ§

S

sÌ‚0 , sÌ‚0 = {Ë†
Ï‚ 0 | Ï‚Ë† âˆˆ sÌ‚ âˆ§ Ï‚Ë†

âˆ§

Î£

Ï‚Ë†0 } âˆª {IÌ‚(e0 )}

This analysis is approximate but remains incomputable because
the stack can grow without bound. Put another way, the height of
the lattice (SÌ‚, âˆª, âˆ©) is infinite and so no finite number of ( S )iterations is guaranteed to obtain a fixed point.

\
gÌ‚ âˆˆ GÌ‚ , VÌ‚ Ã— EÌ‚ Ã— Store

[Dyke graph]

vÌ‚ âˆˆ VÌ‚ , P(Q)

[Dyke vertices]

d
qÌ‚ âˆˆ QÌ‚ , Exp Ã— Env

[Dyke configs.]

\ Â± Ã— QÌ‚)
eÌ‚ âˆˆ EÌ‚ , P(QÌ‚ Ã— Frame

[Dyke edges]

\ Â± , Frame
\ Ã— {push, pop}
Ï†Ì‚Â± âˆˆ Frame

[edge actions]

For readability, we style an edge (qÌ‚, (Ï†Ì‚, push), qÌ‚ 0 ) âˆˆ eÌ‚ like so:

âˆ§

Ï†Ì‚+

qÌ‚ âˆ’â†’ qÌ‚ 0 âˆˆ eÌ‚
3.2

Store-Widened Unbounded-Stack Analysis

It would be too verbose to formalize all the machinery required
to compute a valid Dyke state graph. Instead, we define it from a
Ë† The function DSG : ÎÌ‚ â†’
completed unbounded-stack analysis Î¾.
GÌ‚ produces a Dyke state graph from a fixed-point Î¾Ë† for ( Î ).
Ë† is a valid Dyke state graph analysis for
The graph gÌ‚ = DSG(Î¾)
a program e0 when Î¾Ë† is the unbounded-stack analysis of e0 .

As we will be comparing this unbounded-stack analysis to our new
technique using precise store-allocated continuations, we derive a
global-store-widened version as before:
\
Î¾Ë† âˆˆ ÎÌ‚ , RÌ‚ Ã— Store
b
rÌ‚ âˆˆ RÌ‚ , P(C )

[reachable configs.]

d Ã— Kont
[
cÌ‚ âˆˆ CÌ‚ , Exp Ã— Env

[configurations]

âˆ§

[state-spaces]

Î¾Ì‚

A widened transfer function ( Î ) is defined in terms of ( Î£ ) in
exactly the same manner as ( ~Î ) was derived from ( ~Î£ ) except
that we now have only a single global value store and no continuation store:
âˆ§

(
(rÌ‚, ÏƒÌ‚)

âˆ§

Î

âˆ§

Î

âˆ§

vÌ‚ = {(e, ÏÌ‚) | (e, ÏÌ‚, ÎºÌ‚) âˆˆ rÌ‚}
Ï†Ì‚+

eÌ‚ = {(e, ÏÌ‚) âˆ’â†’ (e0 , ÏÌ‚0 ) | (e, ÏÌ‚, ÎºÌ‚) âˆˆ rÌ‚
âˆ§ (e, ÏÌ‚, ÏƒÌ‚, ÎºÌ‚)

) : ÎÌ‚ â†’ ÎÌ‚
0

Ï†Ì‚

âˆ§

Î£

(e0 , ÏÌ‚0 , ÏƒÌ‚, Ï†Ì‚ : ÎºÌ‚)}

âˆ’

âˆª {(e, ÏÌ‚) âˆ’â†’ (e0 , ÏÌ‚0 ) | (e, ÏÌ‚, ÎºÌ‚) âˆˆ rÌ‚

0

(rÌ‚ , ÏƒÌ‚ ), where

sÌ‚ = {Ë†
Ï‚ | (e, ÏÌ‚, ÎºÌ‚) âˆˆ rÌ‚ âˆ§ (e, ÏÌ‚, ÏƒÌ‚, ÎºÌ‚)

âˆ§

Î£

âˆ§ (e, ÏÌ‚, ÏƒÌ‚, Ï†Ì‚ : ÎºÌ‚)

âˆ§

Î£

(e0 , ÏÌ‚0 , ÏƒÌ‚, ÎºÌ‚)}

Although we do not formalize transition relations for Dyke
state graphs themselves, it will be helpful for us to illustrate the
major source of additional complexity in engineering a PDCFA
directly. In the finite-state analysis, a transition is able to trivially
compute a set of stacks by looking up the current continuation
address in the continuation store. In the unbounded-stack analysis,
a transition is able to trivially compute the stack by looking at the
final component of the state or configuration being transitioned.
In a Dyke state graph, canceling sequences of pushes and pops
may place the set of topmost stack frames on edges arbitrarily
distant from the configuration qÌ‚ being transitioned. In this way,
the implicitness of stacks in a Dyke state graph obfuscates one of
the most common operations needed to compute the analysis (i.e.,
stack introspection). As an example, observe how the topmost stack
frame Ï†Ì‚0 for qÌ‚3 is located elsewhere in the graph:

Ï‚Ë†} âˆª {IÌ‚(e0 )}

rÌ‚0 = {(e, ÏÌ‚, ÎºÌ‚) | (e, ÏÌ‚, ÏƒÌ‚, ÎºÌ‚) âˆˆ sÌ‚}
G
ÏƒÌ‚ 0 =
ÏƒÌ‚ 00
(_,_,ÏƒÌ‚ 00 ,_)âˆˆsÌ‚

3.3

gÌ‚

z }| {
z }| {
DSG((rÌ‚, ÏƒÌ‚)) , (vÌ‚, eÌ‚, ÏƒÌ‚), where

Pushdown Control-Flow Analysis (PDCFA)

Pushdown control-flow analysis (PDCFA) is a strategy for creating
a computable equivalent to the precision of our unbounded-stack
machine at a quadratic-factor increase to the complexity class of the
underlying finite analysis (e.g., monovariant or 1-call-sensitive) [4].
This strategy tracks both reachable states (or in the store-widened
case, configurations) as well as push or pop edges between them. A
quadratic blow up comes from the fact that each pair of reachable
states may have an explicitly-tracked edge between them. These
edges implicitly represent, as possible paths through the graph,
the stacks explicitly represented in the unbounded-stack machine.
This graph precisely describes the regular expression of all stacks
reachable in the pushdown states of the unbounded-stack analysis.
PDCFA formalizes a Dyke state graph for this. Where a sequence of pushes may be repeated ad infinitum, a Dyke state graph
explicitly represents a cycle of push edges and a cycle of pop edges
finitely. Broadly speaking, this is also how AAC and our adaptive
continuation allocator work, except that such cycles are represented
in the store instead of the state graph. A Dyke state graph is a state
transition graph where each edge is annotated with either a frame
push, a frame pop, or an epsilon. The set of continuations for a particular state in a Dyke state graph is determined by the pushes and
pops along the paths that reach that state.
To formalize these Dyke state graphs, we reuse some components of our unbounded-stack machine, continuing to use hats as

qÌ‚0

Ï†Ì‚+
0

/ qÌ‚1

Ï†Ì‚+
1

/ qÌ‚2

Ï†Ì‚âˆ’
1

/ qÌ‚3

PDCFA therefore requires a non-trivial algorithm for stack introspection [5] and extra analysis machinery overall. Specifically,
PDCFA requires the inductive maintenance of an epsilon closure
graph in addition to the Dyke state graph as seen in the following.


qÌ‚0

Ï†Ì‚+
0

/ qÌ‚1

Ï†Ì‚+
1

/ qÌ‚2

Ï†Ì‚âˆ’
1


/ qÌ‚3

This structure makes all sequences of canceling stack actions explicit as an epsilon edge. As we will see, this epsilon closure graph
represents unnecessary additional complexity for both computer
and analysis developer.

7

3.4

Precise Allocation of Continuations (AAC)

Abstracting abstract control (AAC) [8] is another polynomial-time
method for obtaining perfect stack precision. This technique works
by store-allocating continuations using addresses unique enough
to ensure no spurious merging and, like PDCFA, does not require
foreknowledge of the polyvariance (e.g., context sensitivity) being used in the value store. The method is worse than PDCFAâ€™s
quadratic-factor increase in run-time complexity. In the monovariant and store-widened case, its authors believe it to be in O(n8 ) [7].
However, AAC makes perfect stack precision available for free in
terms of development cost (i.e., labor).
Given the standard finite-state abstraction we built up in section 2.2, we can define AACâ€™s essential strategy in a single line:

cÌƒ0

cÌƒ2
f

cÌƒ3

cÌƒ4

cÌƒ5

g

The configuration cÌƒ0 represents the entry point to the function, and
its incoming edge is a call-site transition. The configuration cÌƒ5 represents an exit point for the function, and its outgoing edge is a
return-point transition. A transition where one intraprocedural configuration follows another, like cÌƒ0 â†’ cÌƒ1 , is not technically possible in our restricted ANF language but in more general languages
would be. The functionâ€™s body may call other functions f and g
whose configurations are not a part of the same intraprocedural set
of nodes. The primary insight behind our technique is that a set
of intraprocedural configurations (like cÌƒ0 through cÌƒ5 ) necessarily
share the exact same set of genuine continuations (in this example,
the incoming call-sites for cÌƒ0 ).
We call the set of configurations cÌƒ0 through cÌƒ5 an intraprocedural group because they are those configurations that represent the
body of a function for a single abstract invocationâ€”defined by an
entry point unique to some e and ÏÌƒ. Our central insight is to notice
that this idea of an intraprocedural group also corresponds to those
configurations that share a single set of continuations. Our finitestate machine represents this set of continuations with a continuation address, so if this continuation address is precise enough to
uniquely determine an intraprocedural groupâ€™s entry point (e and
ÏÌƒ), then it can be used for all configurations in that same group.
Thus our allocator may be defined as simply:

] Îº AAC ((e, ÏÌƒ, ÏƒÌƒ, ÎºÌƒ), e0 , ÏÌƒ0 , ÏƒÌƒ 0 ) , (e0 , ÏÌƒ0 , e, ÏÌƒ, ÏƒÌƒ)
alloc
That is, continuations are stored at an address unique to the target
stateâ€™s expression e0 and environment ÏÌƒ0 as well as the source stateâ€™s
expression e, environment ÏÌƒ, and store ÏƒÌƒ.
We have simplified AAC slightly and translated its notation
to give this definition in the terms of our framework. A more
faithful presentation of AAC shows fundamental differences between their framework and ours. AAC uses an eval-apply semantics and explodes each flow set into a set of distinct states
across every application. The exact address AAC proposes using is
f ÏƒÌƒ) (Figure 7 in [8]) where ((Î» (y) e0 ), ÏÌƒÎ» )
(((Î» (y) e0 ), ÏÌƒÎ» ), clo,
f is one particular abstract
is the target closure of an application, clo
closure flowing to y, and ÏƒÌƒ is the value store in the source state.
Our components e0 and ÏÌƒ0 are isomorphic to the target closure in
the sense that e0 is identical and ÏÌƒ0 is produced from the combination of ÏÌƒÎ» and y. The source stateâ€™s components (e, ÏÌƒ, ÏƒÌƒ) are
f and ÏƒÌƒ, but they do uniquely determine a flow
not as specific as clo
f However,
set dËœ (the result of AÌƒ invoked on f ) that contains clo.
a semantics using an eval-apply factoring like AAC is needed to
obtain a unique continuation address for every closure propagated
across an application. This would have significantly complicated
our presentation of the finite-state analysis, and in section 4 we
f adds run-time complexity to an
will see that being specific to clo
analysis without adding any precision.
The intuition for AAC is that by allocating continuations specific to both the source state and target state of a call-site transition,
no merging may occur when returning according to this (transitionspecific) continuation-address. If we were to add some arbitrary
additional context sensitivity (e.g., 3-call-sensitivity), this informa] Îº AAC upon protion would be encoded in ÏÌƒ0 and inherited by alloc
ducing an address. Including this target-state binding environment
in continuation addresses is the key reason why AAC allocates precise continuation addresses.
In section 4, we will see that only the target stateâ€™s expression
e0 and environment ÏÌƒ0 are truly necessary for obtaining the perfect
stack precision of our unbounded-stack machine. Including components of the transitionâ€™s source state, its store, or its flow set only
adds run-time complexity that is unnecessary for achieving perfect
stack precision. This optimization extends AACâ€™s core insight to be
computationally for free while remaining precise and developmentally for free.

] Îº P4F ((e, ÏÌƒ, ÏƒÌƒ, ÏƒÌƒÎº , aÌƒÎº ), e0 , ÏÌƒ0 , ÏƒÌƒ 0 ) = (e0 , ÏÌƒ0 )
alloc
The impact of this change is easily missed, belied by its simplicity. We allocate a continuation based only on the expression and
environment at the entry point of each intraprocedural sequence of
let-forms and it is precisely reinstated when each of the calls in
these let-forms return.
Recall that the monovariant continuation allocator in our example from section 2.5 resulted in return-flow merging because a single continuation address was being used for transitions to multiple
entry points of different intraprocedural groups. More generally,
return-flow merging occurs in a finite-state analysis when, at some
return-point configuration (Ã¦, ÏÌƒÃ¦ , aÌƒÎº ), the set of continuations for
aÌƒÎº is less precise than the set of source configurations that transition to the entry point (e, ÏÌƒ) of the same intraprocedural group.
Because we allocate a continuation address specific to this exact
entry point, and because that address is propagated by shallowly
copying it to each return point for the same intraprocedural group,
the set of continuations will be as precise as the set of source configurations transitioning to the same entry point in all cases. This
means the return-flow merging problem cannot occur when using
] Îº P4F and neither is there a run-time overhead for stack introalloc
spection.
In section 5, we formalize these intuitions and provide a proof
that our unbounded-stack analysis simulates (i.e., is no more pre] Îº P4F .
cise than) a finite-state analysis when using alloc
4.1

4.

cÌƒ1

Complexity

To see why this allocation scheme leads to only a constant-factor
overhead, consider a set of configurations cÌƒ0 , cÌƒ1 , Â· Â· Â· , cÌƒn that form
an intraprocedural group and a set of call sites transitioning to cÌƒ0
with the continuations ÎºÌƒ0 , ÎºÌƒ1 , Â· Â· Â· , ÎºÌƒmâˆ’1 . We can diagrammatically visualize this as the following.

Perfect Stack Precision for Free

The primary intuition of our work can be illustrated by considering
a set of intraprocedural configurations for some function invocation
as in the following with cÌƒ0 through cÌƒ5 .

8

ÎºÌƒ0
cÌƒ1 Â· Â· Â·

cÌƒnâˆ’1

cÌƒn

4.2

Constant Overhead Requires Store Widening

That no function can have two entry points that lead to the same
exit point is a genuine restriction worth discussing further. If this
were not true, our technique would be precise (assuming multiple
entry points are not merged), but it would not necessarily be a
constant-factor increase in complexity. The combination of no store
widening (per-state value stores) and mutation is a good example
of how this situation could arise.
To see how per-state stores can cause a further blow up in
complexity, consider a function that is called with two different
continuations and two different stores. Without store widening,
each store causes a different state to be created for the entry point
(e, ÏÌƒ) of the function. In the following diagram for example, Ï‚Ëœ1 is
the state for the entry point with one store and Ï‚Ëœ10 the state for the
entry point with another store.

âˆ’1

ÎºÌƒm

cÌƒ0

ÎºÌƒ m

âˆ’1

ÎºÌƒ1
..
.

also be imported into the PDCFA style of analysis to yield a variant of PDCFA that incurs only a constant-factor overhead, but this
would require additional machinery.

ÎºÌƒ 0
ÎºÌƒ1
..
.

Note that, for each call site, there is a corresponding return flow
using the same continuation. Our allocation strategy means that
all of the configurations cÌƒ0 , cÌƒ1 , Â· Â· Â· , cÌƒn use the same continuation
address (e, ÏÌƒ). The global continuation store then maps this address
to the set {ÎºÌƒ0 , ÎºÌƒ1 , Â· Â· Â· , ÎºÌƒmâˆ’1 }.
Now consider what must be done if a new call site transitions
to c0 . First, the continuation store must be extended to contain the
continuation for this new call site, say ÎºÌƒm , in the continuation set at
the address (e, ÏÌƒ). Then the corresponding return edge transitions
must be added. Note that none of cÌƒ0 , cÌƒ1 , Â· Â· Â· , cÌƒnâˆ’1 need to be
modified or accessed. The only work done here beyond that of the
underlying analysis is the extension of the continuation store by
adding ÎºÌƒm at (e, ÏÌƒ) and the addition of a corresponding return edge
at return points. Thus, the additional work is a constant factor of the
number of times a continuation is added to the continuation store.
A naÃ¯ve analysis might lead us to conclude that this is bounded
by the product of the number of continuation addresses and the
number of continuations. However, there is a tighter bound. Each
transition adds only one continuation to the continuation store.
Thus the work done is a constant factor of the number of transitions
in the underlying analysis.
Note that this differs from AAC, which may make duplicate
copies of the continuation set for an intraprocedural group as it produces one for each combination of components e, ÏÌƒ, and ÏƒÌƒ drawn
from the source states transitioning to it. As a consequence, AAC
allocates addresses strictly more unique than the target (e0 , ÏÌƒ0 ) configuration. Two different source expressions e0 and e1 may both
have transitions to (e0 , ÏÌƒ0 ), but AAC will produce two different
target configurations cÌƒ00 and cÌƒ01 because the continuation addresses
they allocate will be distinct. This difference is maintained through
the two variants of the function starting at e0 with environment ÏÌƒ0 ,
and when an exit point Ã¦ is reached for each, the expression and
its environment are the same and propagate the same values to two
sets of continuations. Thus, these continuation addresses and the
sets of stacks they represent are kept separate without any benefit.
PDCFA, on the other hand, is more complex for an entirely different reason: the epsilon closure graph. Without the epsilon closure graph, PDCFA has no way to efficiently determine a topmost
stack frame at each return transition. Both our method and AACâ€™s
method make this trivial by propagating an address explicitly to
each state. While our method allows a continuation address to be
shallowly propagated across each intraprocedural node in a function, the epsilon closure graph recomputes a separate set of incoming epsilon edges for every node. This means that the number of
such edges for a given entry point (e, ÏÌ‚) is the number of callers
times the number of intraprocedural nodes. This is a quadratic
blow up from the number of nodes in a finite-state model. This
is why monovariant store-widened PDCFA is in O(n6 ) instead of
in O(n3 ) like traditional 0-CFA. We are able to naturally exploit
our insight that each intraprocedural node following an entry point
(e, ÏÌ‚) shares the same set of continuations (i.e., the same epsilon
edges) by propagating a pointer to this set instead of rebuilding
it for each node. PDCFA is unable to exploit this insight without
adding machinery to propagate only a shallow copy of an incoming
epsilon edge set intraprocedurally. It is likely that this insight could

ÎºÌƒ1
Ï‚Ëœ2

Ï‚Ëœ3

Ï‚Ëœ4

Ï‚Ëœ10

Ï‚Ëœ20

Ï‚Ëœ30

Ï‚Ëœ40

ÎºÌƒ 2

Ï‚Ëœ1

f
Now suppose that along both sequences of states there is a call to
some function f and that f contains a side effect that causes the
previously different value stores to become equal. For example, in
Ï‚Ëœ1 perhaps the address for x maps to {#t} and in Ï‚Ëœ10 it maps to {#f}.
If x becomes bound to {#t, #f} along both paths in the body of f ,
the stores along both paths would become identical.
A problem now arises. Should f return only to one state using
this common store such as Ï‚Ëœ3 or should it return to two different
states (with identical stores) such as both Ï‚Ëœ3 and Ï‚Ëœ30 ? Either choice
has drawbacks. The semantics we have given would naturally yield
the latter option, producing two distinct states that differ only by
their continuation addresses (their original entry point). Because
these states are otherwise identical, splitting ÎºÌƒ1 and ÎºÌƒ2 into sets
represented by two different continuation addresses results in additional transitions and complexity without any benefit. Arguably,
these continuation sets should be merged and represented by a single address. This corresponds to the former option and could save
on run-time complexity but only at the cost of additional analysis machinery. This means per-state stores are incompatible with
our goal of obtaining perfect stack precision for free in both senses
(running time and human labor).
4.3

Implementation

We have implemented both our technique and AACâ€™s technique for
analysis of a simplified Scheme intermediate language. This language extends Exp with a variety of additional core forms including
conditionals, mutation, recursive binding, tail calls, and a library of
primitive operations. Our implementation was written in Scala and
executed using Scala 2.11 for OSX on an Intel Core i5 (1.3 GHz)
with 4GB of RAM. It is built upon the implementation of Earl et
al. [5], which implements both traditional k-CFA and PDCFA. The
test cases we ran came from the Larceny R6RS benchmark suite
(ack, cpstak, tak) and examples compiled from the previous litera-

9

500

AAC Configurations
AAC States

P4F Configurations
P4F States

AAC Configurations
AAC States

2,000

P4F Configurations
P4F States

400
1,500
300
1,000
200
500

100

ture on obtaining perfect stack precision (mj09, eta, kcfa2, kcfa3,
blur, loop2, sat). As a sanity check, we have verified that both AAC
and our method produce results of equivalent precision in every
case. We ran each comparison using both a monovariant value-store
allocator (figure 2), and a 1-call-sensitive polyvariant allocator (figure 3). Across the board, our method requires visiting strictly fewer
machine configurations. In some of these cases the difference is
rather small, but in others it is significant. We saw as much as a
16.0Ã— improvement in the monovariant analysis and as much as
a 10.4Ã— improvement in the context-sensitive analysis. The mean
speedup in terms of states visited was 5.4Ã— and 4.9Ã— in the monovariant and context-sensitive analyses, respectively.

âˆ§

Î£

Î¾Ë†0 and Î¾Ëœ

tak

cpstak

ack

sat

loop2

blur

kcfa3

Lemma 10

cÌ‚0 ,â†’ (e, HEnv (ÏÌƒ), HKont (ÏˆÌƒ))

Lemma 8

(e, HEnv (ÏÌƒ), HKont (ÏˆÌƒ)) âˆˆ rÌ‚

Figure 4. The logical chain proving lemma 10. Assumes (rÌ‚, ÏƒÌ‚) is
at a fixed point and (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº ) is well-formed.

(lemma 5) and for which we can show that any well-formed Î¾Ëœ is
precise relative to any Î¾Ë† that is at a fixed point (lemmas 9 and 10).
The well-formedness property is defined in terms of two additional concepts. First, we formally define the stacks, ÏˆÌƒ, implied by
a continuation address, aÌƒÎº , and continuation store, ÏƒÌƒÎº , in terms of
a relation ÏˆÌƒ âˆˆÏˆ aÌƒÎº (via ÏƒÌƒÎº ) that we define. Then, we define paths,
~
(,â†’) and (,â†’), through Î¾Ë† and Î¾Ëœ in terms of a sequence of state steps,
( Î£ ) and ( Î£~ ), between states represented by configurations in Î¾Ë†
Ëœ This allows us to prove the precision of any well-formed Î¾Ëœ
and Î¾.
(i.e., lemma 10) through a logical chain informally shown in figure 4. In lemma 6, we show that for any configuration (e, ÏÌƒ, aÌƒÎº )
in the rÌƒ of a Î¾Ëœ and any ÏˆÌƒ implied by aÎº with the continuation store
Ëœ there exists a path from the initial configuration cÌƒ0 with an
ÏƒÎº of Î¾,
empty stack  to the configuration (e, ÏÌƒ, aÌƒÎº ) with the implied stack
ÏˆÌƒ. In lemma 7, we then show that there exists a corresponding path
in Î¾Ë† from cÌ‚0 to (e, HEnv (ÏÌƒ), HKont (ÏˆÌƒ)). Finally, in lemma 8, we show
that the endpoint of that path is in Î¾Ë† and thus the set of reachable
configurations in Î¾Ëœ is precise relative to Î¾Ë† (lemma 10).
âˆ§

âˆ§

~ Ëœ0

Î£

(e, ÏÌƒ, aÌƒÎº ) âˆˆ rÌƒ
ÏˆÌƒ âˆˆÏˆ aÌƒÎº (via ÏƒÌƒÎº )

Lemma 7

Proving soundness is fairly straight forward as discussed in section 2.3, but proving precision poses a greater challenge. To do this,
we first define a simulation relation (wÎ ) where Î¾Ë† wÎ Î¾Ëœ (read as
Ëœ if and only if all stored values and machine conâ€œÎ¾Ë† simulates Î¾â€)
figurations in Î¾Ëœ (including stacks implicit in this configuration) are
Ë† Usually, the
accounted for in the unbounded-stack representation Î¾.
next step in such a proof would be to show that taking parallel steps
preserves precision as in fallacy 1.
âˆ§

Lemma 6

~

(cÌƒ0 , ) ,â†’ ((e, ÏÌƒ, aÌƒÎº ), ÏˆÌƒ)

Proof of Precision

Fallacy 1 (Steps preserve precision). If Î¾Ë†
Î¾Ë† wÎ Î¾Ëœ implies Î¾Ë†0 wÎ Î¾Ëœ0 .

kcfa2

mj09

cpstak

ack

sat

loop2

blur

kcfa3

kcfa2

eta

mj09

tak

Figure 3. A 1-call-sensitive comparison.

Figure 2. A monovariant comparison.

5.

eta

0

0

Î¾ , then

However, fallacy 1 is not true. This is because after some finite
number of steps Î¾Ëœ may contain a cycle in its continuation store. This
means that an infinite family of successively longer stacks must
also be in Î¾Ë† for precision to hold. After a finite number of steps,
however, all stacks in Î¾Ë† are bounded by a finite length. Hence, there
are stacks that precision says should be in Î¾Ë† that are not.
We thus take a different approach to proving precision. Before
going into the details, the high-level overview of this proof is as
follows. Instead of stepping both Î¾Ë† and Î¾Ëœ in parallel, we show that
successive steps of Î¾Ëœ are all precise relative to any Î¾Ë† that is already
at a fixed point (i.e., theorem 11 found at the end of this section).
To show this, we need two inductions. One is over the steps taken
Ëœ and the other is over the stacks implied by Î¾.
Ëœ To separate these
by Î¾,
inductions, we define a well-formedness property (wf in figure 7)
that we can show is preserved by iterative steps from an initial Î¾Ëœ0

5.1

Definitions and Assumptions

In order to prove precision, we first require that the address spaces
for both ÏƒÌ‚ and ÏƒÌƒ correspond as follows.
Assumption 2 (Address equivalence). There exists an equiva] ) and
lence (â‰¡Addr ) between finite-state-machine addresses (Addr
[ ) that can be decomunbounded-stack-machine addresses (Addr
[ TAddr Addr
].
posed into a bijection Addr
HAddr

10

~

((e, ÏÌƒ, aÌƒÎº ), ÏˆÌƒ) ,â†’ ((e, ÏÌƒ, aÌƒÎº ), ÏˆÌƒ) (via (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº ), (rÌƒ0 , ÏƒÌƒ 0 , ÏƒÌƒÎº0 )), where
ÏˆÌƒ âˆˆÏˆ aÌƒÎº (via ÏƒÌƒÎº0 )

(e, ÏÌƒ, aÌƒÎº ) âˆˆ rÌƒ0

~

((e, ÏÌƒ, aÌƒÎº ), ÏˆÌƒ) ,â†’ ((e0 , ÏÌƒ0Îº , aÌƒ0Îº ), ÏˆÌƒ 0 ) (via (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº ), (rÌƒ0 , ÏƒÌƒ 0 , ÏƒÌƒÎº0 )), where
]
ÏÌƒ0Îº = ÏÌƒÎº [x 7â†’ alloc(x,
(Ã¦, ÏÌƒ00 , ÏƒÌƒ, ÏƒÌƒÎº , aÌƒ00Îº ))]
00

(Ã¦, ÏÌƒ , aÌƒ00Îº ) âˆˆ rÌƒ

(e0 , ÏÌƒ0Îº , aÌƒ0Îº ) âˆˆ rÌƒ0

((x, e0 , ÏÌƒÎº ), aÌƒ0Îº ) âˆˆ ÏƒÌƒÎº (aÌƒ00Îº )

~

((e, ÏÌƒ, aÌƒÎº ), ÏˆÌƒ) ,â†’ ((Ã¦, ÏÌƒ00 , aÌƒ00Îº ), ((x, e0 , ÏÌƒÎº ), aÌƒ0Îº ) : ÏˆÌƒ 0 ) (via (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº ), (rÌƒ0 , ÏƒÌƒ 0 , ÏƒÌƒÎº0 ))
(Ã¦, ÏÌƒ00 , ÏƒÌƒ, ÏƒÌƒÎº , aÌƒ00Îº )

v

~
Î£

(e0 , ÏÌƒ0Îº , ÏƒÌƒ 0 , ÏƒÌƒÎº0 , aÌƒ0Îº )

~

((e, ÏÌƒ, aÌƒÎº ), ÏˆÌƒ) ,â†’ ((e0 , ÏÌƒ0 , aÌƒ0Îº ), ((x, e00 , ÏÌƒ00 ), aÌƒ00Îº ) : ÏˆÌƒ 0 ) (via (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº ), (rÌƒ0 , ÏƒÌƒ 0 , ÏƒÌƒÎº0 )), where
((x, e00 , ÏÌƒ00 ), aÌƒ00Îº ) âˆˆ ÏƒÌƒÎº0 (aÌƒ0Îº )

((e, ÏÌƒ, aÌƒÎº ), ÏˆÌƒ) âˆˆ rÌƒ
~

((e, ÏÌƒ, aÌƒÎº ), ÏˆÌƒ) ,â†’ (((let ([x (f Ã¦)]) e00 ), ÏÌƒ00 , aÌƒ00Îº ), ÏˆÌƒ 0 ) (via (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº ), (rÌƒ0 , ÏƒÌƒ 0 , ÏƒÌƒÎº0 ))
((let ([x (f Ã¦)]) e00 ), ÏÌƒ00 , ÏƒÌƒ, ÏƒÌƒÎº , aÌƒ00Îº )

v

~
Î£

(e0 , ÏÌƒ0 , ÏƒÌƒ 0 , ÏƒÌƒÎº0 , aÌƒ0Îº )

Figure 5. Finite-state paths
From this we can build up equivalences (â‰¡Env , â‰¡F rame , â‰¡Clo ) and
precision relations (wÎ , wStore , wR , wD , wStore ) for all the components
of our machine. In addition, we can define conversion bijections
(HEnv , TEnv , HF rame , TF rame , HClo , TClo , HD , TD , HStore , TStore ) for most
but not all of the components. These relations have the following
signatures.
(wÎ ) âŠ† ÎÌ‚ Ã— ÎÌƒ

âˆ§

(e, ÏÌ‚, ÎºÌ‚) ,â†’ (e, ÏÌ‚, ÎºÌ‚) (via rÌ‚, ÏƒÌ‚)
(e, ÏÌ‚, ÎºÌ‚) ,â†’ (e0 , ÏÌ‚0 , ÎºÌ‚0 ) (via rÌ‚, ÏƒÌ‚), where
âˆ§

(e, ÏÌ‚, ÎºÌ‚) ,â†’ (e00 , ÏÌ‚00 , ÎºÌ‚00 ) (via rÌ‚, ÏƒÌ‚)
âˆ§

(e00 , ÏÌ‚00 , ÏƒÌ‚, ÎºÌ‚00 )

[state-space precision]

\ Ã— Store
^
(wStore ) âŠ† Store

[store precision]

^
(wR ) âŠ† RÌ‚ Ã— RÌƒ Ã— KStore
d
g
(â‰¡Env ) âŠ† Env Ã— Env

[reachable configs. precision]

\ Ã— Frame
^
(â‰¡F rame ) âŠ† Frame

[frame equivalence]

(wD ) âŠ† DÌ‚ Ã— DÌƒ
d Ã— Clo
g
(â‰¡Clo ) âŠ† Clo

[env. equivalence]

Î£

(e0 , ÏÌ‚0 , ÏƒÌ‚, ÎºÌ‚0 )

(Ï†Ìƒ, aÌƒ0Îº ) âˆˆ ÏƒÌƒÎº (aÌƒÎº ) âˆ§ ÏˆÌƒ âˆˆÏˆ aÌƒ0Îº (via ÏƒÌƒÎº ) âˆ§ aÌƒÎº 6= aÌƒhalt
=â‡’ ((Ï†Ìƒ, aÌƒ0Îº ) : ÏˆÌƒ) âˆˆÏˆ aÌƒÎº (via ÏƒÌƒÎº )

[flow-set precision]

Then given such a ÏˆÌƒ, we can directly construct its equivalent
unbounded stack:

[closure equivalence]

HKont () , 

HKont ((Ï†Ìƒ, aÌƒÎº ) : ÏˆÌƒ) , HF rame (Ï†Ìƒ) : HKont (ÏˆÌƒ)

Also, given a finite-state configuration cÌƒ and an implicit stack ÏˆÌƒ,
we can construct an unbounded-stack configuration cÌ‚:

Assumption 3 (Allocation equivalence). If ÏÌ‚ â‰¡Env ÏÌƒ, ÏƒÌ‚ wStore ÏƒÌƒ,
and ÏˆÌƒ âˆˆÏˆ aÌƒÎº (via ÏƒÌƒÎº ), then:

HC ((e, ÏÌƒ, aÌƒÎº ), ÏˆÌƒ) , (e, HEnv (ÏÌƒ), HKont (ÏˆÌƒ))

[
]
alloc(x,
(e, ÏÌ‚, ÏƒÌ‚, HKont (ÏˆÌƒ))) â‰¡Addr alloc(x,
(e, ÏÌƒ, ÏƒÌƒ, ÏƒÌƒÎº , aÌƒÎº ))

Next, in figures 5 and 6 , we define paths to configurations. For
(,â†’) this is defined by a base case from a configuration to itself and
a recursive case that builds onto an existing path with a step within
Ë† This uses a variation of the step relation defined in figure 8 that
Î¾.
allows the output store of the step to be a sub-store of the store in
~
Ë† The (,â†’)
Î¾.
relation is similar but adds extra side conditions that
ensure invariants used in our proof.
Then, in figure 7, we define well-formedness. This is a binary
predicate with the first argument Î¾Ëœ being the predecessor of the
second argument Î¾Ëœ0 , which is the result we say is well-formed.
This predicate is defined in terms of several sub-properties. The
wf Î¾Ìƒ property requires Î¾Ëœ be well-formed and the predecessor of Î¾Ëœ0 .
The wf v property requires that Î¾Ëœ0 be component-wise greater
Ëœ The wf
than or equal to Î¾.
init and wf halt properties respectively
âˆ§

This assumption uses (âˆˆÏˆ ) and HKont , which deal with the stacks
implied by an address and continuation store. We define an implied
stack as an unbounded list of finite-state continuations ÎºÌƒ:
âˆ—

âˆ§

Figure 6. Unbounded-stack paths

In addition, with the following assumption, we require that the
value allocators respect the address correspondence.

]
ÏˆÌƒ âˆˆ Î¨Ìƒ = Kont

v

[implied stack]

These ÏˆÌƒ are an intermediate representation in that, like ÎºÌ‚, their
structure is unbounded, but each element is taken directly from the
finite-state machine. We define a trinary relation (âˆˆÏˆ ) that specifies
which ÏˆÌƒ are implied by an aÌƒÎº in ÏƒÌƒÎº . This has the following base
case and inductive case:
 âˆˆÏˆ aÌƒhalt (via ÏƒÌƒÎº )

11

Ëœ Î¾Ëœ0 ) , wf (Î¾,
Ëœ Î¾Ëœ0 ) âˆ§ wf (Î¾,
Ëœ Î¾Ëœ0 ) âˆ§ wf (Î¾,
Ëœ Î¾Ëœ0 ) âˆ§ wf (Î¾,
Ëœ Î¾Ëœ0 )
wf (Î¾,
v
init
halt
Î¾Ìƒ
Ëœ Ëœ0

Ëœ Ëœ0

v

Ëœ Ëœ0

âˆ§wf rÌƒ (Î¾, Î¾ ) âˆ§ wf ÏƒÌƒ (Î¾, Î¾ ) âˆ§ wf ÏƒÌƒÎº (Î¾, Î¾ )

(e, ÏÌ‚, ÏƒÌ‚, ÎºÌ‚)

âˆ§

(e, ÏÌ‚, ÏƒÌ‚, ÎºÌ‚)

âˆ§

(e0 , ÏÌ‚0 , ÏƒÌ‚ 00 , ÎºÌ‚0 )

v

~ Ëœ0

Î

Î£

(e0 , ÏÌ‚0 , ÏƒÌ‚ 0 , ÎºÌ‚0 ), where

ÏƒÌ‚ 00 v ÏƒÌ‚ 0

Ëœ Î¾Ëœ0 ) , (Î¾Ëœ = Î¾Ëœ0 = ({(e0 , âˆ…, aÌƒhalt )}, âŠ¥, âŠ¥))
wf Î¾Ìƒ (Î¾,
âˆ¨ (Î¾Ëœ

Î£

Ëœ
Î¾ âˆ§ âˆƒÎ¾Ëœ00 . wf (Î¾Ëœ00 , Î¾))

(e, ÏÌƒ, ÏƒÌƒ, ÏƒÌƒÎº , aÌƒÎº )

~
Î£

(e0 , ÏÌƒ0 , ÏƒÌƒ 0 , ÏƒÌƒÎº0 , aÌƒ0Îº ), where

0
0
000
~
, ÏƒÌƒÎº000 , aÌƒ0Îº )
Î£ (e , ÏÌƒ , ÏƒÌƒ
00
000
0
v ÏƒÌƒ) âˆ§ (ÏƒÌƒÎº v ÏƒÌƒÎº ) âˆ§ (ÏƒÌƒ v ÏƒÌƒ ) âˆ§ (ÏƒÌƒÎº000 v ÏƒÌƒÎº0 )

(e, ÏÌƒ, ÏƒÌƒ 00 , ÏƒÌƒÎº00 , aÌƒÎº )

wf v ((rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº ), (rÌƒ0 , ÏƒÌƒ 0 , ÏƒÌƒÎº0 )) , (rÌƒ âŠ† rÌƒ0 ) âˆ§ (ÏƒÌƒ v ÏƒÌƒ 0 ) âˆ§ (ÏƒÌƒÎº v ÏƒÌƒÎº0 )
(ÏƒÌƒ

00

Ëœ (rÌƒ0 , ÏƒÌƒ 0 , ÏƒÌƒÎº0 )) , (e0 , âˆ…, aÌƒhalt ) âˆˆ rÌƒ0
wf init (Î¾,
v

(e, ÏÌƒ, ÏƒÌƒ, ÏƒÌƒÎº , aÌƒÎº )

Ëœ (rÌƒ0 , ÏƒÌƒ 0 , ÏƒÌƒÎº0 )) , âˆ€ÎºÌƒ. ÎºÌƒ âˆˆ
wf halt (Î¾,
/ ÏƒÌƒÎº0 (aÌƒhalt )
0

(e, ÏÌƒ, ÏƒÌƒ

0

Ëœ (rÌƒ , ÏƒÌƒ , ÏƒÌƒÎº0 )) ,
wf rÌƒ (Î¾,

00

~
Î£

, ÏƒÌƒÎº00 , aÌƒÎº )

~
Î£

f where
(e0 , ÏÌƒ0 , ÏƒÌƒ 0 , ÏƒÌƒÎº0 , aÌƒ0Îº ) (with aÌƒ, clo),
(e0 , ÏÌƒ0 , ÏƒÌƒ 000 , ÏƒÌƒÎº000 , aÌƒ0Îº )

f âˆˆ ÏƒÌƒ 000 (aÌƒ)
(ÏƒÌƒ 00 v ÏƒÌƒ) âˆ§ (ÏƒÌƒÎº00 v ÏƒÌƒÎº ) âˆ§ (ÏƒÌƒ 000 v ÏƒÌƒ 0 ) âˆ§ (ÏƒÌƒÎº000 v ÏƒÌƒÎº0 ) âˆ§ clo

âˆ€(e, ÏÌƒ, aÌƒÎº ) âˆˆ rÌƒ . âˆƒÏˆÌƒ âˆˆÏˆ aÌƒÎº (via ÏƒÌƒÎº0 ).
0

v

~

Ëœ (rÌƒ0 , ÏƒÌƒ 0 , ÏƒÌƒÎº0 ))
((e0 , âˆ…, aÌƒhalt ), ) ,â†’ ((e, ÏÌƒ, aÌƒÎº ), ÏˆÌƒ) (via Î¾,

(e, ÏÌƒ, ÏƒÌƒ, ÏƒÌƒÎº , aÌƒÎº )
(e, ÏÌƒ, ÏƒÌƒ 00 , ÏƒÌƒÎº00 , aÌƒÎº )

wf ÏƒÌƒ ((rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº ), (rÌƒ0 , ÏƒÌƒ 0 , ÏƒÌƒÎº0 )) ,

v

~
Î£

Î£

~
Î£

(e0 , ÏÌƒ0 , ÏƒÌƒ 0 , ÏƒÌƒÎº0 , aÌƒ0Îº ) (with ÎºÌƒ, aÌƒ00Îº ), where
(e0 , ÏÌƒ0 , ÏƒÌƒ 000 , ÏƒÌƒÎº000 , aÌƒ0Îº )

(ÏƒÌƒ 00 v ÏƒÌƒ) âˆ§ (ÏƒÌƒÎº00 v ÏƒÌƒÎº ) âˆ§ (ÏƒÌƒ 000 v ÏƒÌƒ 0 ) âˆ§ (ÏƒÌƒÎº000 v ÏƒÌƒÎº0 ) âˆ§ ÎºÌƒ âˆˆ ÏƒÌƒÎº000 (aÌƒ00Îº )

f âˆˆ ÏƒÌƒ 0 (aÌƒ). âˆƒ(e, ÏÌƒ, aÌƒÎº ) âˆˆ rÌƒ. âˆƒ(e0 , ÏÌƒ0 , aÌƒ0Îº ) âˆˆ rÌƒ0 .
âˆ€aÌƒ. âˆ€clo
(e, ÏÌƒ, ÏƒÌƒ, ÏƒÌƒÎº , aÌƒÎº )

~

Figure 8. Sub-step Relations

f
(e0 , ÏÌƒ0 , ÏƒÌƒ 0 , ÏƒÌƒÎº0 , aÌƒ0Îº ) (with aÌƒ, clo)

wf ÏƒÌƒÎº ((rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº ), (rÌƒ0 , ÏƒÌƒ 0 , ÏƒÌƒÎº0 )) ,

âˆ§ âˆƒf. âˆƒÃ¦. ((let ([x (f Ã¦)]) e), ÏÌƒÎº , aÌƒÎº ) âˆˆ rÌƒ

Ëœ (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº )), and
Assumption 4 (Allocation consistency). If wf (Î¾,
~
(e0 , ÏÌƒ0 [x 7â†’ aÌƒ], ÏƒÌƒ 00 , ÏƒÌƒÎº00 , aÌƒ0Îº )
the state step (e, ÏÌƒ, ÏƒÌƒ, ÏƒÌƒÎº , aÌƒÎº )
Î£
]
holds where aÌƒ = alloc(x,
(e, ÏÌƒ, ÏƒÌƒ, ÏƒÌƒÎº , aÌƒÎº )) and there is a result
step (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº ) Î~ (rÌƒ0 , ÏƒÌƒ 0 , ÏƒÌƒÎº0 ), then the corresponding allocation
for e, ÏÌƒ, and aÌƒÎº , but with ÏƒÌƒ 0 and ÏƒÌƒÎº0 , is the same:

âˆ§ ((let ([x (f Ã¦)]) e), ÏÌƒÎº , ÏƒÌƒ, ÏƒÌƒÎº , aÌƒÎº )

]
]
alloc(x,
(e, ÏÌƒ, ÏƒÌƒ, ÏƒÌƒÎº , aÌƒÎº )) = alloc(x,
(e, ÏÌƒ, ÏƒÌƒ 0 , ÏƒÌƒÎº0 , aÌƒÎº ))

âˆ€aÌƒ0Îº . âˆ€((x, e, ÏÌƒÎº ), aÌƒÎº ) âˆˆ ÏƒÌƒÎº0 (aÌƒ0Îº ).
âˆƒe0Îº . âˆƒÏÌƒ0Îº . aÌƒ0Îº = (e0Îº , ÏÌƒ0Îº ) âˆ§ Entry(aÌƒ0Îº ) âˆˆ rÌƒ0

v

~
Î£

(e0Îº , ÏÌƒ0Îº , ÏƒÌƒ 0 , ÏƒÌƒÎº0 , aÌƒ0Îº ) (with aÌƒ0Îº , ((x, e, ÏÌƒÎº ), aÌƒÎº ))

5.2

Lemmas and Theorems

Ëœ
To start, lemma 5 shows that iterated steps produce well-formed Î¾.
] â†’ CÌƒ
Entry : Addr

Lemma 5 (Well-formedness of analysis results). If Î¾Ëœ0 is the result
of taking zero or more steps of ( Î~ ), starting from the initial result,
Ëœ Î¾Ëœ0 ) for some Î¾.
Ëœ
({(e0 , âˆ…, aÌƒhalt )}, âŠ¥, âŠ¥), then wf (Î¾,

Entry(aÌƒhalt ) , (e0 , âˆ…, aÌƒhalt )
Entry((eÎº , ÏÌƒÎº )) , (eÎº , ÏÌƒÎº , (eÎº , ÏÌƒÎº ))

Proof. We induct over ( Î~ ) steps. In the base case, we can easily
show that the initial result is well-formed. We can also show that
Ëœ Î¾Ëœ0 ) then wf (Î¾Ëœ0 , Î¾Ëœ00 ). This is done using
for any Î¾Ëœ0 Î~ Î¾Ëœ00 , if wf (Î¾,
sublemmas for the components of well-formedness. We omit these
for space.

Figure 7. Well-formedness properties

require that the initial configuration be in Î¾Ëœ0 and that the haltcontinuation address ahalt not have any continuations associated
with it. Finally, wf rÌƒ , wf ÏƒÌƒ , and wf ÏƒÌƒÎº ensure that everything in the
rÌƒ, ÏƒÌƒ, and ÏƒÌƒÎº for Î¾Ëœ0 has a reason to be there. For wf rÌƒ , this means
that every element of rÌƒ has some path leading to it. For wf ÏƒÌƒ and
wf ÏƒÌƒÎº , this means that every value stored in ÏƒÌƒ or ÏƒÌƒÎº has some step
( Î£~ ) that put it there. These are defined in terms of the variations

Next, with lemma 6, we show that every configuration paired
with one of its implied stacks has a path leading to it (i.e., the top
edge of figure 4).
Lemma 6 (Stacks have paths). If (e, ÏÌƒ, aÌƒÎº ) âˆˆ rÌƒ such that
Ëœ (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº )), then:
wf (Î¾,

v

âˆ€ÏˆÌƒ âˆˆÏˆ aÌƒÎº (via ÏƒÌƒÎº ).

of ( ~Î£ ) in figure 8 that have side conditions about the contents of
the store.
For wf ÏƒÌƒÎº , we define Entry, which maps a continuation address to the configuration that is the entry point for the function
invocation that contains the configurations using that continuation
address.
Finally, with the following assumption, we require that once
allocation creates an address it must always produce the same
address for the same configuration even if the value or continuation
stores have changed.

~
Ëœ (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº ))
((e0 , âˆ…, aÌƒhalt ), ) ,â†’ ((e, ÏÌƒ, aÌƒÎº ), ÏˆÌƒ) (via Î¾,

Ëœ (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº )), there is some ÏˆÌƒ 0 âˆˆÏˆ aÌƒÎº (via ÏƒÌƒÎº )
Proof. By wf rÌƒ (Î¾,
~
Ëœ (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº )).
for which ((e0 , âˆ…, aÌƒhalt ), ) ,â†’ ((e, ÏÌƒ, aÌƒÎº ), ÏˆÌƒ 0 ) (via Î¾,
0
However, this path uses ÏˆÌƒ instead of our desired ÏˆÌƒ. Thus we induct
over ÏˆÌƒ. If ÏˆÌƒ is the empty list, , then aÌƒÎº must be aÌƒhalt and thus
 is the only ÏˆÌƒ for which ÏˆÌƒ âˆˆÏˆ aÌƒÎº (via ÏƒÌƒÎº ). So ÏˆÌƒ 0 = ÏˆÌƒ = ,
Ëœ (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº )) equals our desired
and the path obtained from wf rÌƒ (Î¾,
conclusion.

12

If ÏˆÌƒ is ((x, eÎº , ÏÌƒÎº ), aÌƒ0Îº ) : ÏˆÌƒ 00 for some x, eÎº , ÏÌƒÎº , aÌƒ0Îº , ÏˆÌƒ 00 , then
there is a path for ÏˆÌƒ 0 from Entry(aÌƒÎº ) to (e, ÏÌƒ, aÌƒÎº ). By another
induction there is a similar path for ÏˆÌƒ:
~
Ëœ (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº ))
(Entry(aÌƒÎº ), ÏˆÌƒ) ,â†’ ((e, ÏÌƒ, aÌƒÎº ), ÏˆÌƒ) (via Î¾,

Then, with lemma 8, we show that the endpoint of any path in Î¾Ë†
is in Î¾Ë† (i.e., the bottom edge of figure 4).

Ëœ (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº )), there exist f and Ã¦ for a call site
By wf rÌƒ (Î¾,
((let ([x (f Ã¦)]) eÎº ), ÏÌƒÎº , aÌƒ0Îº ) âˆˆ rÌƒ and a step from that call
site to Entry(aÌƒÎº ):

Proof. Trivial. By induction.

((let ([x (f Ã¦)]) eÎº ), ÏÌƒÎº , ÏƒÌƒ, ÏƒÌƒÎº , aÌƒ0Îº )
0

v

~
Î£

Lemma 8 (Path endpoint). If (rÌ‚, ÏƒÌ‚) Î£ (rÌ‚, ÏƒÌ‚), then for any path
cÌ‚0 ,â†’ (e, ÏÌ‚, ÎºÌ‚) (via rÌ‚, ÏƒÌ‚), we have: (e, ÏÌ‚, ÎºÌ‚) âˆˆ rÌ‚.
âˆ§

âˆ§

Finally, with lemmas 9 and 10, we show that precision is preserved by the step relation Î£~ (i.e., the right edge of figure 4).
Then in theorem 11, we show that these are all precise, which is
ultimately what we want to prove.

(e0 , ÏÌƒ0 , ÏƒÌƒ, ÏƒÌƒÎº , aÌƒÎº )

0

(e , ÏÌƒ , aÌƒÎº ) = Entry(aÌƒÎº )

where

Lemma 9 (Preservation of precision for value stores). If (rÌ‚, ÏƒÌ‚) Î
Ëœ (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº )), (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº ) ~Î (rÌƒ0 , ÏƒÌƒ 0 , ÏƒÌƒÎº0 ), and (rÌ‚, ÏƒÌ‚) wÎ
(rÌ‚, ÏƒÌ‚), wf (Î¾,
(rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº ), then ÏƒÌ‚ wStore ÏƒÌƒ 0 .
âˆ§

By the induction hypothesis, we have a path for ÏˆÌƒ 00 from (e0 , âˆ…, aÌƒhalt )
to the call site:
~

((e0 , âˆ…, aÌƒhalt ), ) ,â†’ (((let ([x (f Ã¦)]) eÎº ), ÏÌƒÎº , aÌƒ0Îº ), ÏˆÌƒ 00 )

Proof. Omitted for space.

We now have a path from ((e0 , âˆ…, aÌƒhalt ), ) to the call site with ÏˆÌƒ 00 ,
a step from the call site to Entry(aÌƒÎº ) that pushes (x, eÎº , ÏÌƒÎº ) onto
the stack, and a path from Entry(aÌƒÎº ) to (e, ÏÌƒ, aÌƒÎº ) with ÏˆÌƒ. From
these we can then construct the path desired in our conclusion.

Lemma 10 (Preservation of precision for reachable configura~
Ëœ (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº )), (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº )
tions). If (rÌ‚, ÏƒÌ‚)
(rÌ‚, ÏƒÌ‚), wf (Î¾,
Î
Î
(rÌƒ0 , ÏƒÌƒ 0 , ÏƒÌƒÎº0 ), and (rÌ‚, ÏƒÌ‚) wÎ (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº ), then rÌ‚ wR rÌƒ0 .

Next, with lemma 7, we show that every path in a well-formed
Î¾Ëœ has a corresponding path in any Î¾Ë† that is at at fixed point (i.e., the
left edge of figure 4).

Proof. If we unfold the definition of (wR ), we must show that for all
(e, ÏÌ‚, ÎºÌ‚) âˆˆ rÌ‚0 and ÏˆÌƒ âˆˆÏˆ aÌƒÎº (via ÏƒÌƒÎº0 ), that (e, HEnv (ÏÌƒ), HKont (ÏˆÌƒ)) âˆˆ
rÌ‚. By lemma 6, we have:

âˆˆ

Lemma 7 (Path conversion). If (e, ÏÌƒ, aÌƒÎº )
Ëœ (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº )) and (rÌ‚, ÏƒÌ‚) Î (rÌ‚, ÏƒÌ‚), then:
wf (Î¾,

âˆ§

~
Ëœ (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº ))
((e0 , âˆ…, aÌƒhalt ), ) ,â†’ ((e, ÏÌƒ, aÌƒÎº ), ÏˆÌƒ) (via Î¾,

rÌƒ such that

âˆ§

From this, by lemma 7, we have:

~

Ëœ (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº ))
((e0 , âˆ…, aÌƒhalt ), ) ,â†’ ((e, ÏÌƒ, aÌƒÎº ), ÏˆÌƒ) (via Î¾,

âˆ§

(e0 , âˆ…, ) ,â†’ (e, HEnv (ÏÌƒ), HKont (ÏˆÌƒ)) (via rÌ‚, ÏƒÌ‚)

âˆ§

=â‡’ (e0 , âˆ…, ) ,â†’ HC ((e, ÏÌƒ, aÌƒÎº ), ÏˆÌƒ) (via rÌ‚, ÏƒÌ‚)

Finally, by lemma 8, we have our conclusion.
Theorem 11 (Precision of analysis results). If Î¾Ëœ is the result of taking zero or more steps of ( Î~ ), starting from ({(e0 , âˆ…, aÌƒhalt )}, âŠ¥, âŠ¥),
Ë† then Î¾Ë† wÎ Î¾.
Ëœ
and Î¾Ë† Î Î¾,

Proof. By induction over the finite-state path. We have three cases.
Case: The path is empty. Trivial.
Case: The last step of the path is a return. For some Ã¦, ÏÌƒ0 , aÌƒÎº ,
aÌƒ0Îº , and ÏÌƒ00 , there is a step (Ã¦, ÏÌƒ0 , ÏƒÌƒ, ÏƒÌƒÎº , aÌƒ0Îº )
and, by the induction hypothesis, a path:

v

~
Î£

âˆ§

(e, ÏÌƒ, ÏƒÌƒ, ÏƒÌƒÎº , aÌƒÎº )

Proof. By induction over the number of steps, trivial simplifications, unfoldings, and lemmas 5, 9, and 10.

(e0 , âˆ…, ) ,â†’ HC ((Ã¦, ÏÌƒ0 , aÌƒ0Îº ), ((x, e, ÏÌƒ00 ), aÌƒÎº ) : ÏˆÌƒ) (via rÌ‚, ÏƒÌ‚)
]
where
ÏÌƒ = ÏÌƒ00 [x 7â†’ alloc(x,
(Ã¦, ÏÌƒ0 , ÏƒÌƒ, ÏƒÌƒÎº , aÌƒ0Îº ))]
âˆ§

6.

Conclusion

(e0 , âˆ…, ) ,â†’ HC (((let ([x (f Ã¦)]) e0 ), ÏÌƒ0 , aÌƒ0Îº ), ÏˆÌƒ 0 ) (via rÌ‚, ÏƒÌ‚)

Traditional control-flow analysis has long suffered from returnflow conflation of values, even when context sensitivity and related techniques keep these values separate across function calls.
Recent approaches have made significant progress in addressing
this problem. However, each suffers from serious drawbacks. PDCFA incurs a substantial development cost and causes a quadraticfactor increase in run-time complexity. AAC is trivial to implement,
but incurs an even worse increase in run-time complexity. Our approach, however, both is simple to implement and adds no asymptotic cost to run-time complexity. To accomplish this, we synthesize
the lessons learned from PDCFA and AAC to show that the ideal
continuation address is simply a functionâ€™s polyvariant entry point:
its expression and abstract binding environment. This introspection
on entry points and the corresponding choice of continuation address yields a finite-state analysis whose call transitions are precisely matched with return transitions at no cost to either run-time
or development-time overhead.

We can then show that (rÌ‚, ÏƒÌ‚) contains a step corresponding to the
step in (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº ):

Acknowledgments

We can then show that (rÌ‚, ÏƒÌ‚) contains a step corresponding to the
step in (rÌƒ, ÏƒÌƒ, ÏƒÌƒÎº ):
(Ã¦, HEnv (ÏÌƒ0 ), ÏƒÌ‚, (x, e, HEnv (ÏÌƒ00 )) : HKont (ÏˆÌƒ))
v

~
Î£

(e, HEnv (ÏÌƒ), ÏƒÌ‚, HKont (ÏˆÌƒ))

Combining this with the path from the induction hypothesis, we
can then construct the path in our conclusion.
Case: The last step of the path is a call. For some x, y, f , Ã¦, e0 ,
ÏÌƒ0 , ÏÌƒÎ» , aÌƒÎº , ÏˆÌƒ 0 , we have (y, e, ÏÌƒÎ» ) âˆˆ AÌƒ(f, ÏÌƒ0 , ÏƒÌƒ), and a step:
v

((let ([x (f Ã¦)]) e0 ), ÏÌƒ0 , ÏƒÌƒ, ÏƒÌƒÎº , aÌƒ0Îº ) Î£~ (e, ÏÌƒ, ÏƒÌƒ, ÏƒÌƒÎº , aÌƒÎº ), where
]
ÏÌƒ = ÏÌƒÎ» [x 7â†’ alloc(x,
((let ([x (f Ã¦)]) e0 ), ÏÌƒ0 , ÏƒÌƒ, ÏƒÌƒÎº , aÌƒ0Îº ))]
and, by the induction hypothesis, a path:
âˆ§

This material is partially based on research sponsored by DARPA
under agreement numbers AFRL FA8750-15-2-0092 and FA875012-2-0106 and by NSF under CAREER grant 1350344. The U.S.
Government is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright notation
thereon.

((let ([x (f Ã¦)]) e0 ), HEnv (ÏÌƒ0 ), ÏƒÌ‚, HKont (ÏˆÌƒ 0 ))
v

~
Î£

(e, HEnv (ÏÌƒ), ÏƒÌ‚, (x, e0 , HEnv (ÏÌƒ0 )) : HKont (ÏˆÌƒ 0 ))

Combining this with the path from the induction hypothesis, we
can then construct the path in our conclusion.

13

References

[9] J. I. Johnson, N. Labich, M. Might, and D. Van Horn. Optimizing abstract abstract machines. In Proceedings of the 18th ACM SIGPLAN
International Conference on Functional Programming, ICFP â€™13,
pages 443â€“454, New York, NY, USA, Sept. 2013. ACM. ISBN 9781-4503-2326-0. doi: 10.1145/2500365.2500604.

[1] P. Cousot and R. Cousot. Static determination of dynamic properties
of programs. In Proceedings of the Second International Symposium
on Programming, pages 106â€“130. Paris, France, 1976.
[2] P. Cousot and R. Cousot. Abstract interpretation: a unified lattice
model for static analysis of programs by construction or approximation of fixpoints. In Proceedings of the 4th ACM SIGACT-SIGPLAN
Symposium on Principles of Programming Languages, POPL â€™77,
pages 238â€“252, New York, NY, USA, Jan. 1977. ACM. doi: 10.1145/
512950.512973.
[3] P. Cousot and R. Cousot. Systematic design of program analysis
frameworks. In Proceedings of the 6th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages, POPL â€™79, pages
269â€“282, New York, NY, USA, Jan. 1979. ACM. doi: 10.1145/
567752.567778.
[4] C. Earl, M. Might, and D. Van Horn. Pushdown control-flow analysis
of higher-order programs: Precise, polyvariant and polynomial-time.
In Scheme Workshop, August 2010.
[5] C. Earl, I. Sergey, M. Might, and D. Van Horn. Introspective pushdown analysis of higher-order programs. In Proceedings of the 17th
ACM SIGPLAN International Conference on Functional Programming, ICFP â€™12, pages 177â€“188, New York, NY, USA, Sept. 2012.
ACM. ISBN 978-1-4503-1054-3. doi: 10.1145/2364527.2364576.
[6] C. Flanagan, A. Sabry, B. F. Duba, and M. Felleisen. The essence of
compiling with continuations. In Proceedings of the ACM SIGPLAN
1993 Conference on Programming Language Design and Implementation, PLDI â€™93, pages 237â€“247, New York, NY, USA, Aug. 1993.
ACM. ISBN 0-89791-598-4. doi: 10.1145/155090.155113.
[7] J. I. Johnson. AAC complexity analysis discussion. Unpublished
correspondence, June 2015.
[8] J. I. Johnson and D. Van Horn. Abstracting abstract control. In
Proceedings of the 10th ACM Symposium on Dynamic Languages,
DLS â€™14, pages 11â€“22, New York, NY, USA, Oct. 2014. ACM. ISBN
978-1-4503-3211-8. doi: 10.1145/2661088.2661098.

[10] J. Midtgaard. Control-flow analysis of functional programs. ACM
Computing Surveys (CSUR), 44(3):10:1â€“10:33, June 2012. ISSN
0360-0300. doi: 10.1145/2187671.2187672.
[11] M. Might. Environment Analysis of Higher-Order Languages. PhD
thesis, Georgia Institute of Technology, Atlanta, GA, 2007.
[12] M. Might. Abstract interpreters for free. In R. Cousot and M. Martel, editors, Proceedings of the Static Analysis Symposium, volume
6337 of Lecture Notes in Computer Science, pages 407â€“421. Springer
Berlin Heidelberg, 2010. ISBN 978-3-642-15768-4. doi: 10.1007/
978-3-642-15769-1\_25.
[13] M. Might, Y. Smaragdakis, and D. Van Horn. Resolving and exploiting
the k-CFA paradox: illuminating functional vs. object-oriented program analysis. In Proceedings of the 31st ACM SIGPLAN Conference
on Programming Language Design and Implementation, PLDI â€™10,
pages 305â€“315, New York, NY, USA, June 2010. ACM. ISBN 978-14503-0019-3. doi: 10.1145/1806596.1806631.
[14] A. Tarski. A lattice-theoretical fixpoint theorem and its applications.
Pacific Journal of Mathematics, 5(2):285â€“309, 1955.
[15] D. Van Horn and M. Might. Abstracting abstract machines. In
Proceedings of the 15th ACM SIGPLAN International Conference on
Functional Programming, ICFP â€™10, pages 51â€“62, New York, NY,
USA, Sept. 2010. ACM. ISBN 978-1-60558-794-3. doi: 10.1145/
1863543.1863553.
[16] D. Vardoulakis and O. Shivers. CFA2: A context-free approach to
control-flow analysis. In A. D. Gordon, editor, Proceedings of the
European Symposium on Programming, volume 6012 of Lecture Notes
in Computer Science, pages 570â€“589. Springer Berlin Heidelberg,
2010. ISBN 978-3-642-11956-9. doi: 10.1007/978-3-642-11957-6\
_30.

14

